{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.10.0",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# NFL Player Movement Prediction - Big Data Bowl 2026\n\n## Solution Overview\nThis notebook implements a comprehensive solution for predicting NFL player movement during pass plays.\n\n### Architecture\n1. **Transformer Backbone (Time Dimension)**: Sequence model for tracking frames\n2. **GNN Interaction Layer (Space Dimension)**: Graph neural network for player interactions\n3. **Football Features Head (Tabular Brain)**: Engineered features like distances, speeds, angles\n4. **Ensemble**: Blend deep models with GBDT (LightGBM/XGBoost)\n\n### Evaluation Metric\nRoot Mean Squared Error (RMSE) between predicted and observed (x, y) coordinates.\n\n### Data Structure\n- **Input files** (`train/input_2023_w*.csv`): Pre-pass tracking data\n- **Output files** (`train/output_2023_w*.csv`): Post-pass target positions (x, y)\n- **Test files**: `test/test.csv` (prediction targets) and `test/test_input.csv` (input features)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# Section 1: Imports and Configuration\n# ============================================================================\n\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom dataclasses import dataclass, field\nimport pickle\nimport json\nimport glob\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split, KFold, GroupKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import mean_squared_error\n\n# Deep Learning\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR\n\n# Graph Neural Networks\ntry:\n    import torch_geometric\n    from torch_geometric.nn import GCNConv, GATConv, TransformerConv\n    from torch_geometric.data import Data, Batch\n    HAS_TORCH_GEOMETRIC = True\nexcept ImportError:\n    HAS_TORCH_GEOMETRIC = False\n    print(\"torch_geometric not available. GNN features will be disabled.\")\n\n# Gradient Boosting\ntry:\n    import lightgbm as lgb\n    HAS_LIGHTGBM = True\nexcept ImportError:\n    HAS_LIGHTGBM = False\n    print(\"LightGBM not available. Will use XGBoost or skip GBDT.\")\n\ntry:\n    import xgboost as xgb\n    HAS_XGBOOST = True\nexcept ImportError:\n    HAS_XGBOOST = False\n    print(\"XGBoost not available.\")\n\nwarnings.filterwarnings('ignore')\n\n# Configuration\n@dataclass\nclass Config:\n    \"\"\"Configuration for the NFL Movement Prediction model.\"\"\"\n    # Data paths - support both local and Kaggle environments\n    data_dir: str = './train/'  # Local training data directory\n    test_dir: str = './test/'   # Local test data directory\n    kaggle_data_dir: str = '/kaggle/input/nfl-big-data-bowl-2026-prediction/'\n    output_dir: str = './outputs/'\n    \n    # Model parameters\n    random_seed: int = 42\n    n_folds: int = 5\n    \n    # Transformer parameters\n    d_model: int = 128\n    n_heads: int = 8\n    n_encoder_layers: int = 4\n    dim_feedforward: int = 512\n    dropout: float = 0.1\n    max_seq_len: int = 100  # Maximum frames to consider\n    \n    # GNN parameters\n    gnn_hidden_dim: int = 64\n    gnn_num_layers: int = 3\n    gnn_heads: int = 4\n    \n    # Training parameters\n    batch_size: int = 32\n    learning_rate: float = 1e-4\n    weight_decay: float = 1e-5\n    epochs: int = 50\n    patience: int = 10\n    \n    # Feature engineering\n    use_velocity_features: bool = True\n    use_acceleration_features: bool = True\n    use_angle_features: bool = True\n    use_distance_features: bool = True\n    use_separation_features: bool = True\n    \n    # Ensemble weights (will be tuned)\n    transformer_weight: float = 0.4\n    gnn_weight: float = 0.3\n    gbdt_weight: float = 0.3\n    \n    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nconfig = Config()\nprint(f\"Using device: {config.device}\")\nprint(f\"torch_geometric available: {HAS_TORCH_GEOMETRIC}\")\nprint(f\"LightGBM available: {HAS_LIGHTGBM}\")\nprint(f\"XGBoost available: {HAS_XGBOOST}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Section 2: Data Loading and Exploration\n\nThe NFL tracking data contains player positions at 10 frames per second. The data structure:\n\n### Input Files (Pre-pass tracking)\n| Column | Description |\n|--------|-------------|\n| `game_id` | Unique game identifier |\n| `play_id` | Play identifier |\n| `player_to_predict` | Whether this player's x/y will be scored |\n| `nfl_id` | Unique player identifier |\n| `frame_id` | Frame number |\n| `x`, `y` | Player position on field |\n| `s`, `a` | Speed and acceleration |\n| `dir`, `o` | Direction and orientation (degrees) |\n| `ball_land_x`, `ball_land_y` | Ball landing location |\n| `num_frames_output` | Number of frames to predict |\n\n### Output Files (Post-pass targets)\n| Column | Description |\n|--------|-------------|\n| `game_id`, `play_id`, `nfl_id`, `frame_id` | Identifiers |\n| `x`, `y` | **TARGET** player positions |",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# Section 2: Data Loading and Exploration\n# ============================================================================\n\nclass NFLDataLoader:\n    \"\"\"Handles loading and preprocessing of NFL tracking data.\"\"\"\n    \n    def __init__(self, config: Config):\n        self.config = config\n        self.train_dir = Path(config.data_dir)\n        self.test_dir = Path(config.test_dir)\n        \n    def load_training_data(self, weeks: Optional[List[int]] = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"\n        Load training data from input and output files.\n        \n        Args:\n            weeks: List of week numbers to load (1-18). If None, load all.\n            \n        Returns:\n            Tuple of (input_df, output_df)\n        \"\"\"\n        input_dfs = []\n        output_dfs = []\n        \n        # Find all input files\n        input_pattern = str(self.train_dir / 'input_2023_w*.csv')\n        input_files = sorted(glob.glob(input_pattern))\n        \n        if not input_files:\n            print(f\"No input files found at {input_pattern}\")\n            return None, None\n        \n        print(f\"Found {len(input_files)} input files\")\n        \n        for input_file in input_files:\n            # Extract week number\n            week_str = Path(input_file).stem.split('_w')[-1]\n            week = int(week_str)\n            \n            if weeks is not None and week not in weeks:\n                continue\n            \n            # Load input file\n            input_df = pd.read_csv(input_file)\n            input_df['week'] = week\n            input_dfs.append(input_df)\n            print(f\"  Loaded {Path(input_file).name}: {len(input_df):,} rows\")\n            \n            # Load corresponding output file\n            output_file = str(self.train_dir / f'output_2023_w{week_str}.csv')\n            if os.path.exists(output_file):\n                output_df = pd.read_csv(output_file)\n                output_df['week'] = week\n                output_dfs.append(output_df)\n        \n        if not input_dfs:\n            print(\"No data loaded!\")\n            return None, None\n        \n        all_input = pd.concat(input_dfs, ignore_index=True)\n        all_output = pd.concat(output_dfs, ignore_index=True) if output_dfs else None\n        \n        print(f\"\\nTotal input rows: {len(all_input):,}\")\n        if all_output is not None:\n            print(f\"Total output rows: {len(all_output):,}\")\n        \n        return all_input, all_output\n    \n    def load_test_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"\n        Load test data.\n        \n        Returns:\n            Tuple of (test_df, test_input_df)\n        \"\"\"\n        test_file = self.test_dir / 'test.csv'\n        test_input_file = self.test_dir / 'test_input.csv'\n        \n        test_df = None\n        test_input_df = None\n        \n        if test_file.exists():\n            test_df = pd.read_csv(test_file)\n            print(f\"Loaded test.csv: {len(test_df):,} rows\")\n        \n        if test_input_file.exists():\n            test_input_df = pd.read_csv(test_input_file)\n            print(f\"Loaded test_input.csv: {len(test_input_df):,} rows\")\n        \n        return test_df, test_input_df\n    \n    def merge_input_output(self, input_df: pd.DataFrame, output_df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Merge input features with output targets for training.\n        \n        The input contains pre-pass data, output contains post-pass positions.\n        We need to align them properly for supervised learning.\n        \"\"\"\n        # Get the last frame of each player in each play from input (pre-pass state)\n        last_input_frame = input_df.groupby(['game_id', 'play_id', 'nfl_id']).agg({\n            'frame_id': 'max',\n            'x': 'last',\n            'y': 'last',\n            's': 'last',\n            'a': 'last',\n            'dir': 'last',\n            'o': 'last',\n            'ball_land_x': 'first',\n            'ball_land_y': 'first',\n            'num_frames_output': 'first',\n            'player_to_predict': 'first',\n            'play_direction': 'first',\n            'absolute_yardline_number': 'first',\n            'player_position': 'first',\n            'player_side': 'first',\n            'player_role': 'first',\n            'player_height': 'first',\n            'player_weight': 'first'\n        }).reset_index()\n        \n        last_input_frame.rename(columns={\n            'x': 'x_pre',\n            'y': 'y_pre',\n            's': 's_pre',\n            'a': 'a_pre',\n            'dir': 'dir_pre',\n            'o': 'o_pre',\n            'frame_id': 'last_input_frame_id'\n        }, inplace=True)\n        \n        # Merge with output data\n        merged = output_df.merge(\n            last_input_frame,\n            on=['game_id', 'play_id', 'nfl_id'],\n            how='left'\n        )\n        \n        # Target x, y are already in output_df\n        # x_pre, y_pre are the last known positions before the throw\n        \n        return merged\n    \n    def create_training_samples(\n        self, \n        input_df: pd.DataFrame, \n        output_df: pd.DataFrame,\n        include_all_players: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Create training samples by pairing input features with output targets.\n        \n        Args:\n            input_df: Pre-pass tracking data\n            output_df: Post-pass target positions\n            include_all_players: If False, only include players marked for prediction\n        \"\"\"\n        # Get last known state for each player before pass\n        last_state = input_df.sort_values('frame_id').groupby(\n            ['game_id', 'play_id', 'nfl_id']\n        ).last().reset_index()\n        \n        # Rename input columns to avoid confusion with targets\n        feature_cols = ['x', 'y', 's', 'a', 'dir', 'o']\n        rename_dict = {col: f'{col}_input' for col in feature_cols}\n        last_state = last_state.rename(columns=rename_dict)\n        \n        # Merge with targets\n        training_df = output_df.merge(\n            last_state,\n            on=['game_id', 'play_id', 'nfl_id'],\n            how='left'\n        )\n        \n        # Filter to only players we need to predict if requested\n        if not include_all_players and 'player_to_predict' in training_df.columns:\n            training_df = training_df[training_df['player_to_predict'] == True]\n        \n        # Create unique identifier\n        training_df['id'] = (\n            training_df['game_id'].astype(str) + '_' + \n            training_df['play_id'].astype(str) + '_' + \n            training_df['nfl_id'].astype(str) + '_' + \n            training_df['frame_id'].astype(str)\n        )\n        \n        return training_df\n\n\n# Initialize data loader\ndata_loader = NFLDataLoader(config)\n\n# Load training data\nprint(\"Loading training data...\")\ninput_df, output_df = data_loader.load_training_data()\n\nif input_df is not None:\n    print(f\"\\nInput columns: {input_df.columns.tolist()}\")\n    print(f\"\\nInput sample:\")\n    display(input_df.head())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Explore the data structure\nif input_df is not None:\n    print(\"\\n\" + \"=\"*60)\n    print(\"INPUT DATA EXPLORATION\")\n    print(\"=\"*60)\n    \n    print(f\"\\nShape: {input_df.shape}\")\n    print(f\"\\nColumn types:\")\n    print(input_df.dtypes)\n    \n    print(f\"\\nUnique values:\")\n    print(f\"  Games: {input_df['game_id'].nunique()}\")\n    print(f\"  Plays: {input_df.groupby('game_id')['play_id'].nunique().sum()}\")\n    print(f\"  Players: {input_df['nfl_id'].nunique()}\")\n    \n    print(f\"\\nPlayer positions: {input_df['player_position'].value_counts().head(10).to_dict()}\")\n    print(f\"\\nPlayer sides: {input_df['player_side'].value_counts().to_dict()}\")\n    print(f\"\\nPlayer roles: {input_df['player_role'].value_counts().to_dict()}\")\n    \n    print(f\"\\nPlayers to predict per play:\")\n    pred_counts = input_df.groupby(['game_id', 'play_id'])['player_to_predict'].sum()\n    print(f\"  Mean: {pred_counts.mean():.2f}\")\n    print(f\"  Min: {pred_counts.min()}, Max: {pred_counts.max()}\")\n\nif output_df is not None:\n    print(\"\\n\" + \"=\"*60)\n    print(\"OUTPUT DATA EXPLORATION\")\n    print(\"=\"*60)\n    \n    print(f\"\\nShape: {output_df.shape}\")\n    print(f\"\\nColumns: {output_df.columns.tolist()}\")\n    print(f\"\\nSample:\")\n    display(output_df.head())\n    \n    print(f\"\\nFrames per player per play:\")\n    frame_counts = output_df.groupby(['game_id', 'play_id', 'nfl_id'])['frame_id'].count()\n    print(f\"  Mean: {frame_counts.mean():.2f}\")\n    print(f\"  Min: {frame_counts.min()}, Max: {frame_counts.max()}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Section 3: Feature Engineering\n\nWe create football-specific features that capture:\n- **Velocity & Acceleration**: Speed, direction changes\n- **Distance Features**: Distance to ball landing spot, to other players\n- **Angle Features**: Angle to ball, pursuit angles\n- **Separation Features**: Closest defender distance, separation metrics\n- **Context Features**: Field position, time since release",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# Section 3: Feature Engineering\n# ============================================================================\n\nclass FootballFeatureEngineer:\n    \"\"\"Creates football-specific features for player movement prediction.\"\"\"\n    \n    def __init__(self, config: Config):\n        self.config = config\n        self.scalers = {}\n        \n    def compute_velocity_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Compute velocity-based features.\"\"\"\n        df = df.copy()\n        \n        # Use input features (pre-pass state)\n        s_col = 's_input' if 's_input' in df.columns else 's'\n        dir_col = 'dir_input' if 'dir_input' in df.columns else 'dir'\n        \n        if s_col in df.columns and dir_col in df.columns:\n            # Velocity components from speed and direction\n            df['vx'] = df[s_col] * np.cos(np.radians(df[dir_col]))\n            df['vy'] = df[s_col] * np.sin(np.radians(df[dir_col]))\n        \n        return df\n    \n    def compute_distance_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Compute distance-based features.\"\"\"\n        df = df.copy()\n        \n        # Get position columns\n        x_col = 'x_input' if 'x_input' in df.columns else 'x'\n        y_col = 'y_input' if 'y_input' in df.columns else 'y'\n        \n        # Distance to ball landing spot\n        if 'ball_land_x' in df.columns and 'ball_land_y' in df.columns:\n            df['dist_to_ball_landing'] = np.sqrt(\n                (df[x_col] - df['ball_land_x'])**2 + \n                (df[y_col] - df['ball_land_y'])**2\n            )\n        \n        # Distance to line of scrimmage\n        if 'absolute_yardline_number' in df.columns:\n            df['dist_from_los'] = df[x_col] - df['absolute_yardline_number']\n        \n        # Distance to sidelines (field is 53.3 yards wide)\n        if y_col in df.columns:\n            df['dist_to_near_sideline'] = np.minimum(df[y_col], 53.3 - df[y_col])\n            df['dist_from_center'] = np.abs(df[y_col] - 26.65)\n        \n        return df\n    \n    def compute_angle_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Compute angle-based features.\"\"\"\n        df = df.copy()\n        \n        x_col = 'x_input' if 'x_input' in df.columns else 'x'\n        y_col = 'y_input' if 'y_input' in df.columns else 'y'\n        dir_col = 'dir_input' if 'dir_input' in df.columns else 'dir'\n        o_col = 'o_input' if 'o_input' in df.columns else 'o'\n        \n        # Angle to ball landing spot\n        if 'ball_land_x' in df.columns and 'ball_land_y' in df.columns:\n            df['angle_to_ball'] = np.degrees(np.arctan2(\n                df['ball_land_y'] - df[y_col],\n                df['ball_land_x'] - df[x_col]\n            ))\n            \n            if dir_col in df.columns:\n                # Difference between direction and angle to ball (pursuit angle)\n                df['pursuit_angle'] = np.abs(\n                    ((df[dir_col] - df['angle_to_ball'] + 180) % 360) - 180\n                )\n                \n                # Is player facing the ball? (within 45 degrees)\n                df['facing_ball'] = (df['pursuit_angle'] < 45).astype(int)\n        \n        # Orientation vs direction (body alignment)\n        if dir_col in df.columns and o_col in df.columns:\n            df['body_alignment'] = np.abs(\n                ((df[o_col] - df[dir_col] + 180) % 360) - 180\n            )\n        \n        return df\n    \n    def compute_player_context_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Compute player-specific context features.\"\"\"\n        df = df.copy()\n        \n        # Encode player side (offense vs defense)\n        if 'player_side' in df.columns:\n            df['is_offense'] = (df['player_side'] == 'Offense').astype(int)\n            df['is_defense'] = (df['player_side'] == 'Defense').astype(int)\n        \n        # Encode key positions\n        if 'player_position' in df.columns:\n            df['is_receiver'] = df['player_position'].isin(['WR', 'TE']).astype(int)\n            df['is_db'] = df['player_position'].isin(['CB', 'SS', 'FS', 'DB', 'S']).astype(int)\n            df['is_lb'] = df['player_position'].isin(['LB', 'ILB', 'OLB', 'MLB']).astype(int)\n        \n        # Play direction adjustment\n        if 'play_direction' in df.columns:\n            df['play_dir_right'] = (df['play_direction'] == 'right').astype(int)\n        \n        # Frame-based features\n        if 'frame_id' in df.columns and 'num_frames_output' in df.columns:\n            df['frame_progress'] = df['frame_id'] / df['num_frames_output'].clip(lower=1)\n        \n        return df\n    \n    def compute_target_relative_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Compute features relative to target position.\"\"\"\n        df = df.copy()\n        \n        x_col = 'x_input' if 'x_input' in df.columns else 'x'\n        y_col = 'y_input' if 'y_input' in df.columns else 'y'\n        \n        # Distance and angle to target (post-pass position we're predicting)\n        if 'x' in df.columns and 'y' in df.columns and x_col in df.columns:\n            df['target_displacement_x'] = df['x'] - df[x_col]\n            df['target_displacement_y'] = df['y'] - df[y_col]\n            df['target_displacement_dist'] = np.sqrt(\n                df['target_displacement_x']**2 + df['target_displacement_y']**2\n            )\n        \n        return df\n    \n    def engineer_features(\n        self, \n        df: pd.DataFrame, \n        is_training: bool = True\n    ) -> pd.DataFrame:\n        \"\"\"Apply all feature engineering steps.\"\"\"\n        print(\"Engineering features...\")\n        \n        if self.config.use_velocity_features:\n            print(\"  Computing velocity features...\")\n            df = self.compute_velocity_features(df)\n        \n        if self.config.use_distance_features:\n            print(\"  Computing distance features...\")\n            df = self.compute_distance_features(df)\n        \n        if self.config.use_angle_features:\n            print(\"  Computing angle features...\")\n            df = self.compute_angle_features(df)\n        \n        print(\"  Computing player context features...\")\n        df = self.compute_player_context_features(df)\n        \n        if is_training:\n            print(\"  Computing target relative features...\")\n            df = self.compute_target_relative_features(df)\n        \n        print(f\"  Feature engineering complete. Shape: {df.shape}\")\n        return df\n    \n    def get_feature_columns(self) -> List[str]:\n        \"\"\"Return list of feature column names for model input.\"\"\"\n        features = [\n            # Input position/motion features\n            'x_input', 'y_input', 's_input', 'a_input', 'dir_input', 'o_input',\n            # Velocity features\n            'vx', 'vy',\n            # Distance features\n            'dist_to_ball_landing', 'dist_from_los',\n            'dist_to_near_sideline', 'dist_from_center',\n            # Angle features\n            'angle_to_ball', 'pursuit_angle', 'facing_ball', 'body_alignment',\n            # Player context features\n            'is_offense', 'is_defense', 'is_receiver', 'is_db', 'is_lb',\n            'play_dir_right', 'frame_progress',\n            # Ball landing position\n            'ball_land_x', 'ball_land_y',\n            # Frame info\n            'frame_id', 'num_frames_output'\n        ]\n        return features\n    \n    def get_target_columns(self) -> List[str]:\n        \"\"\"Return list of target column names.\"\"\"\n        return ['x', 'y']\n\n\n# Initialize feature engineer\nfeature_engineer = FootballFeatureEngineer(config)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Create training dataset with features\nif input_df is not None and output_df is not None:\n    print(\"Creating training samples...\")\n    \n    # Create training samples by merging input (pre-pass) with output (targets)\n    training_df = data_loader.create_training_samples(\n        input_df, \n        output_df,\n        include_all_players=False  # Only predict for marked players\n    )\n    \n    print(f\"\\nTraining samples created: {len(training_df):,} rows\")\n    print(f\"Columns: {training_df.columns.tolist()}\")\n    \n    # Engineer features\n    training_df = feature_engineer.engineer_features(training_df, is_training=True)\n    \n    print(f\"\\nAfter feature engineering: {training_df.shape}\")\n    \n    # Show sample\n    feature_cols = feature_engineer.get_feature_columns()\n    available_features = [c for c in feature_cols if c in training_df.columns]\n    print(f\"\\nAvailable features ({len(available_features)}): {available_features}\")\n    \n    display(training_df.head())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Section 4: Transformer Model (Time Dimension)\n\nThe Transformer model processes sequences of player tracking data over time:\n- Positional encoding for temporal ordering\n- Multi-head self-attention for capturing temporal dependencies\n- Predicts future (x, y) positions based on past trajectory",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# Section 4: Transformer Model for Temporal Sequences\n# ============================================================================\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Positional encoding for transformer.\"\"\"\n    \n    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n        \n        pe = torch.zeros(max_len, d_model)\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        \n        self.register_buffer('pe', pe)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.pe[:, :x.size(1), :]\n        return self.dropout(x)\n\n\nclass PlayerMovementTransformer(nn.Module):\n    \"\"\"Transformer model for predicting player movement trajectories.\"\"\"\n    \n    def __init__(\n        self,\n        input_dim: int,\n        d_model: int = 128,\n        n_heads: int = 8,\n        n_encoder_layers: int = 4,\n        dim_feedforward: int = 512,\n        dropout: float = 0.1,\n        max_seq_len: int = 100\n    ):\n        super().__init__()\n        \n        self.d_model = d_model\n        \n        # Input projection\n        self.input_proj = nn.Sequential(\n            nn.Linear(input_dim, d_model),\n            nn.LayerNorm(d_model),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        \n        # Positional encoding\n        self.pos_encoder = PositionalEncoding(d_model, max_seq_len, dropout)\n        \n        # Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layer,\n            num_layers=n_encoder_layers\n        )\n        \n        # Output heads for x and y prediction\n        self.output_head = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model // 2, 2)  # Predict (x, y)\n        )\n    \n    def forward(\n        self, \n        x: torch.Tensor, \n        mask: Optional[torch.Tensor] = None\n    ) -> torch.Tensor:\n        # Project input to model dimension\n        x = self.input_proj(x)\n        \n        # Add positional encoding\n        x = self.pos_encoder(x)\n        \n        # Create attention mask if provided\n        if mask is not None:\n            attn_mask = ~mask\n        else:\n            attn_mask = None\n        \n        # Transformer encoding\n        encoded = self.transformer_encoder(x, src_key_padding_mask=attn_mask)\n        \n        # Predict coordinates\n        predictions = self.output_head(encoded)\n        \n        return predictions\n\n\nprint(\"Transformer model architecture defined.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Section 5: Graph Neural Network (Space Dimension)\n\nThe GNN captures player interactions within each frame:\n- Players are nodes with position/velocity features\n- Edges connect players based on proximity or team relationships\n- Message passing aggregates information from nearby players",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# Section 5: Graph Neural Network for Player Interactions\n# ============================================================================\n\nclass PlayerInteractionGNN(nn.Module):\n    \"\"\"Graph Neural Network for modeling player interactions.\"\"\"\n    \n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int = 64,\n        output_dim: int = 2,\n        num_layers: int = 3,\n        heads: int = 4,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        \n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        # Input projection\n        self.input_proj = nn.Linear(input_dim, hidden_dim)\n        \n        # GNN layers - use simple MLP-based message passing\n        self.gnn_layers = nn.ModuleList()\n        for _ in range(num_layers):\n            self.gnn_layers.append(nn.Sequential(\n                nn.Linear(hidden_dim * 2, hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(dropout),\n                nn.Linear(hidden_dim, hidden_dim)\n            ))\n        \n        # Layer normalization\n        self.layer_norms = nn.ModuleList([\n            nn.LayerNorm(hidden_dim) for _ in range(num_layers)\n        ])\n        \n        # Output head\n        self.output_head = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, output_dim)\n        )\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(\n        self,\n        x: torch.Tensor,\n        positions: torch.Tensor,\n        edge_threshold: float = 15.0\n    ) -> torch.Tensor:\n        # Project input\n        h = self.input_proj(x)\n        \n        # Build adjacency based on positions\n        diff = positions.unsqueeze(0) - positions.unsqueeze(1)\n        distances = torch.norm(diff, dim=-1)\n        adj = (distances < edge_threshold).float()\n        adj = adj / (adj.sum(dim=-1, keepdim=True) + 1e-8)\n        \n        # Message passing layers\n        for i, layer in enumerate(self.gnn_layers):\n            neighbor_features = torch.matmul(adj, h)\n            combined = torch.cat([h, neighbor_features], dim=-1)\n            h_new = layer(combined)\n            h = self.layer_norms[i](h + self.dropout(h_new))\n        \n        # Output prediction\n        output = self.output_head(h)\n        return output\n\n\nprint(\"GNN model architecture defined.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Section 6: GBDT Model (Tabular Features)\n\nGradient Boosted Decision Trees for tabular feature processing:\n- LightGBM/XGBoost for fast training\n- Handles engineered features effectively\n- Provides complementary predictions to deep models",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# Section 6: GBDT Model for Tabular Features\n# ============================================================================\n\nclass GBDTPredictor:\n    \"\"\"GBDT model for tabular feature prediction.\"\"\"\n    \n    def __init__(self, config: Config):\n        self.config = config\n        self.model_x = None\n        self.model_y = None\n        self.feature_cols = None\n        self.scaler = StandardScaler()\n        \n    def prepare_features(\n        self, \n        df: pd.DataFrame, \n        feature_cols: List[str]\n    ) -> np.ndarray:\n        \"\"\"Prepare features for GBDT model.\"\"\"\n        self.feature_cols = [c for c in feature_cols if c in df.columns]\n        \n        X = df[self.feature_cols].values\n        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n        \n        return X\n    \n    def train(\n        self, \n        df: pd.DataFrame,\n        feature_cols: List[str],\n        target_col_x: str = 'x',\n        target_col_y: str = 'y',\n        val_df: Optional[pd.DataFrame] = None\n    ):\n        \"\"\"Train GBDT models for x and y prediction.\"\"\"\n        X = self.prepare_features(df, feature_cols)\n        y_x = df[target_col_x].values\n        y_y = df[target_col_y].values\n        \n        # Scale features\n        X_scaled = self.scaler.fit_transform(X)\n        \n        # Prepare validation data if provided\n        X_val_scaled = None\n        y_val_x = None\n        y_val_y = None\n        if val_df is not None:\n            X_val = self.prepare_features(val_df, feature_cols)\n            X_val_scaled = self.scaler.transform(X_val)\n            y_val_x = val_df[target_col_x].values\n            y_val_y = val_df[target_col_y].values\n        \n        if HAS_LIGHTGBM:\n            print(\"Training LightGBM models...\")\n            \n            params = {\n                'objective': 'regression',\n                'metric': 'rmse',\n                'boosting_type': 'gbdt',\n                'num_leaves': 63,\n                'learning_rate': 0.05,\n                'feature_fraction': 0.8,\n                'bagging_fraction': 0.8,\n                'bagging_freq': 5,\n                'verbose': -1,\n                'n_jobs': -1,\n                'random_state': self.config.random_seed\n            }\n            \n            # Train model for X\n            train_data_x = lgb.Dataset(X_scaled, label=y_x)\n            valid_sets_x = [train_data_x]\n            if X_val_scaled is not None:\n                valid_data_x = lgb.Dataset(X_val_scaled, label=y_val_x)\n                valid_sets_x.append(valid_data_x)\n            \n            self.model_x = lgb.train(\n                params,\n                train_data_x,\n                num_boost_round=1000,\n                valid_sets=valid_sets_x,\n                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n            )\n            \n            # Train model for Y\n            train_data_y = lgb.Dataset(X_scaled, label=y_y)\n            valid_sets_y = [train_data_y]\n            if X_val_scaled is not None:\n                valid_data_y = lgb.Dataset(X_val_scaled, label=y_val_y)\n                valid_sets_y.append(valid_data_y)\n            \n            self.model_y = lgb.train(\n                params,\n                train_data_y,\n                num_boost_round=1000,\n                valid_sets=valid_sets_y,\n                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n            )\n            \n        elif HAS_XGBOOST:\n            print(\"Training XGBoost models...\")\n            \n            params = {\n                'objective': 'reg:squarederror',\n                'max_depth': 8,\n                'learning_rate': 0.05,\n                'n_estimators': 1000,\n                'subsample': 0.8,\n                'colsample_bytree': 0.8,\n                'random_state': self.config.random_seed,\n                'n_jobs': -1\n            }\n            \n            eval_set_x = [(X_scaled, y_x)]\n            eval_set_y = [(X_scaled, y_y)]\n            if X_val_scaled is not None:\n                eval_set_x.append((X_val_scaled, y_val_x))\n                eval_set_y.append((X_val_scaled, y_val_y))\n            \n            self.model_x = xgb.XGBRegressor(**params)\n            self.model_x.fit(\n                X_scaled, y_x,\n                eval_set=eval_set_x,\n                verbose=100\n            )\n            \n            self.model_y = xgb.XGBRegressor(**params)\n            self.model_y.fit(\n                X_scaled, y_y,\n                eval_set=eval_set_y,\n                verbose=100\n            )\n        else:\n            print(\"No GBDT library available. Skipping GBDT training.\")\n            return\n        \n        print(\"GBDT training complete.\")\n    \n    def predict(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Generate predictions using GBDT models.\"\"\"\n        if self.model_x is None or self.model_y is None:\n            raise ValueError(\"Models not trained. Call train() first.\")\n        \n        X = self.prepare_features(df, self.feature_cols)\n        X_scaled = self.scaler.transform(X)\n        \n        pred_x = self.model_x.predict(X_scaled)\n        pred_y = self.model_y.predict(X_scaled)\n        \n        return pred_x, pred_y\n    \n    def get_feature_importance(self) -> pd.DataFrame:\n        \"\"\"Get feature importance from trained models.\"\"\"\n        if self.model_x is None:\n            return pd.DataFrame()\n        \n        if HAS_LIGHTGBM:\n            importance_x = self.model_x.feature_importance(importance_type='gain')\n            importance_y = self.model_y.feature_importance(importance_type='gain')\n        elif HAS_XGBOOST:\n            importance_x = self.model_x.feature_importances_\n            importance_y = self.model_y.feature_importances_\n        else:\n            return pd.DataFrame()\n        \n        importance_df = pd.DataFrame({\n            'feature': self.feature_cols,\n            'importance_x': importance_x,\n            'importance_y': importance_y,\n            'importance_avg': (importance_x + importance_y) / 2\n        }).sort_values('importance_avg', ascending=False)\n        \n        return importance_df\n    \n    def save(self, path: str):\n        \"\"\"Save model to disk.\"\"\"\n        os.makedirs(os.path.dirname(path) if os.path.dirname(path) else '.', exist_ok=True)\n        with open(path, 'wb') as f:\n            pickle.dump({\n                'model_x': self.model_x,\n                'model_y': self.model_y,\n                'scaler': self.scaler,\n                'feature_cols': self.feature_cols\n            }, f)\n        print(f\"Model saved to {path}\")\n    \n    def load(self, path: str):\n        \"\"\"Load model from disk.\"\"\"\n        with open(path, 'rb') as f:\n            data = pickle.load(f)\n            self.model_x = data['model_x']\n            self.model_y = data['model_y']\n            self.scaler = data['scaler']\n            self.feature_cols = data['feature_cols']\n        print(f\"Model loaded from {path}\")\n\n\nprint(\"GBDT model class defined.\")\nprint(f\"LightGBM available: {HAS_LIGHTGBM}\")\nprint(f\"XGBoost available: {HAS_XGBOOST}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Section 7: Training Pipeline\n\nComplete training pipeline that:\n1. Splits data into train/validation\n2. Engineers features\n3. Trains GBDT models\n4. Evaluates performance",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# Section 7: Training Pipeline\n# ============================================================================\n\ndef train_gbdt_model(\n    training_df: pd.DataFrame,\n    feature_engineer: FootballFeatureEngineer,\n    config: Config,\n    val_ratio: float = 0.2\n) -> GBDTPredictor:\n    \"\"\"Train the GBDT model on prepared training data.\"\"\"\n    \n    print(\"=\"*60)\n    print(\"Training GBDT Model\")\n    print(\"=\"*60)\n    \n    # Get feature columns\n    feature_cols = feature_engineer.get_feature_columns()\n    available_features = [c for c in feature_cols if c in training_df.columns]\n    print(f\"\\nUsing {len(available_features)} features\")\n    \n    # Split by plays for proper validation\n    unique_plays = training_df[['game_id', 'play_id']].drop_duplicates()\n    train_plays, val_plays = train_test_split(\n        unique_plays, \n        test_size=val_ratio, \n        random_state=config.random_seed\n    )\n    \n    train_df = training_df.merge(train_plays, on=['game_id', 'play_id'])\n    val_df = training_df.merge(val_plays, on=['game_id', 'play_id'])\n    \n    print(f\"\\nTrain size: {len(train_df):,}, Validation size: {len(val_df):,}\")\n    \n    # Initialize and train GBDT\n    gbdt = GBDTPredictor(config)\n    gbdt.train(\n        train_df, \n        available_features,\n        target_col_x='x',\n        target_col_y='y',\n        val_df=val_df\n    )\n    \n    # Evaluate on validation set\n    print(\"\\n=== Validation Evaluation ===\")\n    pred_x, pred_y = gbdt.predict(val_df)\n    \n    rmse_x = np.sqrt(mean_squared_error(val_df['x'], pred_x))\n    rmse_y = np.sqrt(mean_squared_error(val_df['y'], pred_y))\n    rmse_combined = np.sqrt(0.5 * (rmse_x**2 + rmse_y**2))\n    \n    print(f\"Validation RMSE X: {rmse_x:.4f}\")\n    print(f\"Validation RMSE Y: {rmse_y:.4f}\")\n    print(f\"Validation RMSE Combined: {rmse_combined:.4f}\")\n    \n    # Feature importance\n    importance_df = gbdt.get_feature_importance()\n    print(f\"\\nTop 10 Features:\")\n    print(importance_df.head(10).to_string(index=False))\n    \n    return gbdt, val_df, pred_x, pred_y\n\n\n# Train the model\nif 'training_df' in dir() and training_df is not None and len(training_df) > 0:\n    gbdt_model, val_df, val_pred_x, val_pred_y = train_gbdt_model(\n        training_df, \n        feature_engineer, \n        config\n    )\n    \n    # Save the model\n    gbdt_model.save('./model_output/gbdt_model.pkl')\nelse:\n    print(\"No training data available. Please load data first.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Section 8: Inference and Submission\n\nIntegration with the Kaggle evaluation API for submission.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# Section 8: Inference and Submission\n# ============================================================================\n\n# Global model storage\nGLOBAL_GBDT_MODEL = None\nGLOBAL_FEATURE_ENGINEER = None\nGLOBAL_CONFIG = None\nGLOBAL_INITIALIZED = False\n\n\ndef initialize_model():\n    \"\"\"Initialize model for inference.\"\"\"\n    global GLOBAL_GBDT_MODEL, GLOBAL_FEATURE_ENGINEER, GLOBAL_CONFIG, GLOBAL_INITIALIZED\n    \n    if GLOBAL_INITIALIZED:\n        return\n    \n    GLOBAL_CONFIG = Config()\n    GLOBAL_FEATURE_ENGINEER = FootballFeatureEngineer(GLOBAL_CONFIG)\n    GLOBAL_GBDT_MODEL = GBDTPredictor(GLOBAL_CONFIG)\n    \n    # Try to load saved model\n    model_path = './model_output/gbdt_model.pkl'\n    if os.path.exists(model_path):\n        GLOBAL_GBDT_MODEL.load(model_path)\n        print(\"Model loaded successfully.\")\n    else:\n        print(\"No saved model found. Using physics-based baseline.\")\n    \n    GLOBAL_INITIALIZED = True\n\n\ndef predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Main prediction function for the inference server.\n    \n    Args:\n        test: DataFrame with rows to predict (id, game_id, play_id, nfl_id, frame_id)\n        test_input: DataFrame with input features (pre-pass tracking data)\n        \n    Returns:\n        DataFrame with 'x' and 'y' predictions\n    \"\"\"\n    global GLOBAL_GBDT_MODEL, GLOBAL_FEATURE_ENGINEER, GLOBAL_CONFIG\n    \n    # Initialize on first call\n    initialize_model()\n    \n    # Convert to pandas for processing\n    test_pd = test.to_pandas() if isinstance(test, pl.DataFrame) else test\n    test_input_pd = test_input.to_pandas() if isinstance(test_input, pl.DataFrame) else test_input\n    \n    n_rows = len(test_pd)\n    \n    try:\n        # Get the last frame for each player from input (pre-pass state)\n        last_state = test_input_pd.sort_values('frame_id').groupby(\n            ['game_id', 'play_id', 'nfl_id']\n        ).last().reset_index()\n        \n        # Rename input columns\n        feature_cols = ['x', 'y', 's', 'a', 'dir', 'o']\n        rename_dict = {col: f'{col}_input' for col in feature_cols if col in last_state.columns}\n        last_state = last_state.rename(columns=rename_dict)\n        \n        # Merge test with input features\n        merged = test_pd.merge(\n            last_state,\n            on=['game_id', 'play_id', 'nfl_id'],\n            how='left'\n        )\n        \n        # Engineer features\n        merged = GLOBAL_FEATURE_ENGINEER.engineer_features(merged, is_training=False)\n        \n        # Try to use GBDT model\n        if GLOBAL_GBDT_MODEL.model_x is not None:\n            pred_x, pred_y = GLOBAL_GBDT_MODEL.predict(merged)\n        else:\n            # Physics-based baseline\n            dt = 0.1 * merged['frame_id']  # Time since pass release\n            \n            if 's_input' in merged.columns and 'dir_input' in merged.columns:\n                vx = merged['s_input'] * np.cos(np.radians(merged['dir_input']))\n                vy = merged['s_input'] * np.sin(np.radians(merged['dir_input']))\n                pred_x = merged['x_input'] + vx * dt\n                pred_y = merged['y_input'] + vy * dt\n            else:\n                pred_x = merged.get('x_input', np.zeros(n_rows) + 50).values\n                pred_y = merged.get('y_input', np.zeros(n_rows) + 26.65).values\n            \n            pred_x = np.array(pred_x)\n            pred_y = np.array(pred_y)\n        \n        # Clip predictions to field boundaries\n        pred_x = np.clip(pred_x, 0, 120)\n        pred_y = np.clip(pred_y, 0, 53.3)\n            \n    except Exception as e:\n        print(f\"Prediction error: {e}\")\n        import traceback\n        traceback.print_exc()\n        \n        # Fallback predictions (field center)\n        pred_x = np.zeros(n_rows) + 50\n        pred_y = np.zeros(n_rows) + 26.65\n    \n    # Create prediction dataframe\n    predictions = pl.DataFrame({\n        'x': pred_x.tolist(),\n        'y': pred_y.tolist()\n    })\n    \n    assert len(predictions) == len(test), f\"Predictions length {len(predictions)} != test length {len(test)}\"\n    return predictions\n\n\nprint(\"Inference function defined.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# Section 9: Submission Code\n# ============================================================================\n\nimport os\nimport pandas as pd\nimport polars as pl\n\ntry:\n    import kaggle_evaluation.nfl_inference_server\n    HAS_KAGGLE_EVAL = True\nexcept ImportError:\n    HAS_KAGGLE_EVAL = False\n    print(\"kaggle_evaluation not available. Running in development mode.\")\n\n\nif HAS_KAGGLE_EVAL:\n    # Create inference server with our predict function\n    inference_server = kaggle_evaluation.nfl_inference_server.NFLInferenceServer(predict)\n    \n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        # Production mode - serve predictions\n        inference_server.serve()\n    else:\n        # Development mode - run local gateway\n        inference_server.run_local_gateway(\n            ('/kaggle/input/nfl-big-data-bowl-2026-prediction/',)\n        )\nelse:\n    print(\"\\n\" + \"=\"*60)\n    print(\"Development Mode - Testing with Local Test Data\")\n    print(\"=\"*60)\n    \n    # Load test data\n    test_df, test_input_df = data_loader.load_test_data()\n    \n    if test_df is not None and test_input_df is not None:\n        print(f\"\\nTest rows to predict: {len(test_df):,}\")\n        print(f\"Test input rows: {len(test_input_df):,}\")\n        \n        # Convert to polars for prediction function\n        test_pl = pl.DataFrame(test_df)\n        test_input_pl = pl.DataFrame(test_input_df)\n        \n        # Run prediction\n        predictions = predict(test_pl, test_input_pl)\n        \n        print(f\"\\nPredictions shape: {predictions.shape}\")\n        print(f\"\\nSample predictions:\")\n        print(predictions.head())\n        \n        # Save predictions\n        pred_df = predictions.to_pandas()\n        pred_df['id'] = test_df['id']\n        pred_df = pred_df[['id', 'x', 'y']]\n        pred_df.to_csv('./test_predictions.csv', index=False)\n        print(f\"\\nPredictions saved to ./test_predictions.csv\")\n    else:\n        print(\"No test data available.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Section 10: Visualization and Analysis",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# Section 10: Visualization and Analysis\n# ============================================================================\n\ndef plot_field():\n    \"\"\"Create a football field plot.\"\"\"\n    fig, ax = plt.subplots(figsize=(12, 5.33))\n    \n    ax.set_xlim(0, 120)\n    ax.set_ylim(0, 53.3)\n    ax.set_facecolor('#2e7d32')\n    \n    # Yard lines\n    for x in range(0, 121, 10):\n        ax.axvline(x, color='white', linewidth=0.5, alpha=0.5)\n    \n    # End zones\n    ax.axvline(10, color='white', linewidth=2)\n    ax.axvline(110, color='white', linewidth=2)\n    \n    return fig, ax\n\n\ndef plot_prediction_errors(val_df, pred_x, pred_y):\n    \"\"\"Plot prediction error distributions.\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n    \n    error_x = pred_x - val_df['x'].values\n    error_y = pred_y - val_df['y'].values\n    error_dist = np.sqrt(error_x**2 + error_y**2)\n    \n    axes[0].hist(error_x, bins=50, edgecolor='black', alpha=0.7)\n    axes[0].axvline(0, color='red', linestyle='--')\n    axes[0].set_xlabel('X Error (yards)')\n    axes[0].set_title(f'X Error\\nMean: {error_x.mean():.2f}, Std: {error_x.std():.2f}')\n    \n    axes[1].hist(error_y, bins=50, edgecolor='black', alpha=0.7)\n    axes[1].axvline(0, color='red', linestyle='--')\n    axes[1].set_xlabel('Y Error (yards)')\n    axes[1].set_title(f'Y Error\\nMean: {error_y.mean():.2f}, Std: {error_y.std():.2f}')\n    \n    axes[2].hist(error_dist, bins=50, edgecolor='black', alpha=0.7, color='green')\n    axes[2].set_xlabel('Distance Error (yards)')\n    axes[2].set_title(f'Distance Error\\nMean: {error_dist.mean():.2f}, Std: {error_dist.std():.2f}')\n    \n    plt.tight_layout()\n    return fig\n\n\ndef plot_feature_importance(importance_df, top_n=15):\n    \"\"\"Plot feature importance.\"\"\"\n    if len(importance_df) == 0:\n        print(\"No feature importance data available.\")\n        return None\n    \n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    top_features = importance_df.head(top_n)\n    \n    y_pos = np.arange(len(top_features))\n    ax.barh(y_pos, top_features['importance_avg'], align='center', color='steelblue')\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(top_features['feature'])\n    ax.invert_yaxis()\n    ax.set_xlabel('Importance (Gain)')\n    ax.set_title(f'Top {top_n} Feature Importance')\n    \n    plt.tight_layout()\n    return fig\n\n\n# Create visualizations if we have validation data\nif 'val_df' in dir() and 'val_pred_x' in dir():\n    print(\"\\n\" + \"=\"*60)\n    print(\"Model Analysis\")\n    print(\"=\"*60)\n    \n    # Plot error distributions\n    fig = plot_prediction_errors(val_df, val_pred_x, val_pred_y)\n    plt.savefig('./error_distribution.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    # Plot feature importance\n    if 'gbdt_model' in dir():\n        importance_df = gbdt_model.get_feature_importance()\n        fig = plot_feature_importance(importance_df)\n        if fig:\n            plt.savefig('./feature_importance.png', dpi=150, bbox_inches='tight')\n            plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Summary\n\nThis notebook implements a comprehensive NFL player movement prediction solution:\n\n### Components\n1. **Data Loading**: Loads training data from `train/input_*.csv` and `train/output_*.csv`\n2. **Feature Engineering**: Football-specific features including velocity, distance to ball, angles, and player context\n3. **GBDT Model**: LightGBM/XGBoost for efficient tabular prediction\n4. **Inference Pipeline**: Ready for Kaggle evaluation API\n\n### Data Structure\n- **Input**: Pre-pass tracking data (position, velocity, direction before throw)\n- **Output**: Post-pass target positions (x, y to predict)\n- **Key columns**: `game_id`, `play_id`, `nfl_id`, `frame_id`, `ball_land_x`, `ball_land_y`\n\n### Next Steps for Improvement\n1. Add Transformer model for temporal sequence modeling\n2. Add GNN model for player interactions\n3. Implement ensemble of multiple models\n4. Add physics-based constraints\n5. Train position-specific models (WR, CB, etc.)\n6. Add trajectory smoothing",
      "metadata": {}
    }
  ]
}
