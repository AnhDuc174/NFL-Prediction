{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NFL Player Movement Prediction - Big Data Bowl 2026\n",
        "\n",
        "## Solution Overview\n",
        "This notebook implements a comprehensive solution for predicting NFL player movement during pass plays.\n",
        "\n",
        "### Architecture\n",
        "1. **Transformer Backbone (Time Dimension)**: Sequence model for tracking frames\n",
        "2. **GNN Interaction Layer (Space Dimension)**: Graph neural network for player interactions\n",
        "3. **Football Features Head (Tabular Brain)**: Engineered features like distances, speeds, angles\n",
        "4. **Ensemble**: Blend deep models with GBDT (LightGBM/XGBoost)\n",
        "\n",
        "### Evaluation Metric\n",
        "Root Mean Squared Error (RMSE) between predicted and observed (x, y) coordinates.\n",
        "\n",
        "### Data Structure\n",
        "- **Input files** (`train/input_2023_w*.csv`): Pre-pass tracking data\n",
        "- **Output files** (`train/output_2023_w*.csv`): Post-pass target positions (x, y)\n",
        "- **Test files**: `test/test.csv` (prediction targets) and `test/test_input.csv` (input features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "torch_geometric available: True\n",
            "LightGBM available: True\n",
            "XGBoost available: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Section 1: Imports and Configuration\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "from dataclasses import dataclass, field\n",
        "import pickle\n",
        "import json\n",
        "import glob\n",
        "\n",
        "# Visualization\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # Non-interactive backend\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, KFold, GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Deep Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR\n",
        "\n",
        "# Graph Neural Networks\n",
        "try:\n",
        "    import torch_geometric\n",
        "    from torch_geometric.nn import GCNConv, GATConv, TransformerConv\n",
        "    from torch_geometric.data import Data, Batch\n",
        "    HAS_TORCH_GEOMETRIC = True\n",
        "except ImportError:\n",
        "    HAS_TORCH_GEOMETRIC = False\n",
        "    print(\"torch_geometric not available. GNN features will be disabled.\")\n",
        "\n",
        "# Gradient Boosting\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    HAS_LIGHTGBM = True\n",
        "except ImportError:\n",
        "    HAS_LIGHTGBM = False\n",
        "    print(\"LightGBM not available. Will use XGBoost or skip GBDT.\")\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    HAS_XGBOOST = True\n",
        "except ImportError:\n",
        "    HAS_XGBOOST = False\n",
        "    print(\"XGBoost not available.\")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Configuration for the NFL Movement Prediction model.\"\"\"\n",
        "    # Data paths - support both local and Kaggle environments\n",
        "    data_dir: str = './train/'  # Local training data directory\n",
        "    test_dir: str = './test/'   # Local test data directory\n",
        "    kaggle_data_dir: str = '/kaggle/input/nfl-big-data-bowl-2026-prediction/'\n",
        "    output_dir: str = './outputs/'\n",
        "    \n",
        "    # Model parameters\n",
        "    random_seed: int = 42\n",
        "    n_folds: int = 5\n",
        "    \n",
        "    # Transformer parameters\n",
        "    d_model: int = 128\n",
        "    n_heads: int = 8\n",
        "    n_encoder_layers: int = 4\n",
        "    dim_feedforward: int = 512\n",
        "    dropout: float = 0.1\n",
        "    max_seq_len: int = 100  # Maximum frames to consider\n",
        "    \n",
        "    # GNN parameters\n",
        "    gnn_hidden_dim: int = 64\n",
        "    gnn_num_layers: int = 3\n",
        "    gnn_heads: int = 4\n",
        "    \n",
        "    # Training parameters\n",
        "    batch_size: int = 32\n",
        "    learning_rate: float = 1e-4\n",
        "    weight_decay: float = 1e-5\n",
        "    epochs: int = 50\n",
        "    patience: int = 10\n",
        "    \n",
        "    # Feature engineering\n",
        "    use_velocity_features: bool = True\n",
        "    use_acceleration_features: bool = True\n",
        "    use_angle_features: bool = True\n",
        "    use_distance_features: bool = True\n",
        "    use_separation_features: bool = True\n",
        "    \n",
        "    # Ensemble weights (will be tuned)\n",
        "    transformer_weight: float = 0.4\n",
        "    gnn_weight: float = 0.3\n",
        "    gbdt_weight: float = 0.3\n",
        "    \n",
        "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = Config()\n",
        "print(f\"Using device: {config.device}\")\n",
        "print(f\"torch_geometric available: {HAS_TORCH_GEOMETRIC}\")\n",
        "print(f\"LightGBM available: {HAS_LIGHTGBM}\")\n",
        "print(f\"XGBoost available: {HAS_XGBOOST}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Data Loading and Exploration\n",
        "\n",
        "The NFL tracking data contains player positions at 10 frames per second. The data structure:\n",
        "\n",
        "### Input Files (Pre-pass tracking)\n",
        "| Column | Description |\n",
        "|--------|-------------|\n",
        "| `game_id` | Unique game identifier |\n",
        "| `play_id` | Play identifier |\n",
        "| `player_to_predict` | Whether this player's x/y will be scored |\n",
        "| `nfl_id` | Unique player identifier |\n",
        "| `frame_id` | Frame number |\n",
        "| `x`, `y` | Player position on field |\n",
        "| `s`, `a` | Speed and acceleration |\n",
        "| `dir`, `o` | Direction and orientation (degrees) |\n",
        "| `ball_land_x`, `ball_land_y` | Ball landing location |\n",
        "| `num_frames_output` | Number of frames to predict |\n",
        "\n",
        "### Output Files (Post-pass targets)\n",
        "| Column | Description |\n",
        "|--------|-------------|\n",
        "| `game_id`, `play_id`, `nfl_id`, `frame_id` | Identifiers |\n",
        "| `x`, `y` | **TARGET** player positions |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Loading training data...\n",
            "============================================================\n",
            "Found 18 input files\n",
            "  Loaded input_2023_w01.csv: 285,714 rows\n",
            "  Loaded input_2023_w02.csv: 288,586 rows\n",
            "  Loaded input_2023_w03.csv: 297,757 rows\n",
            "  Loaded input_2023_w04.csv: 272,475 rows\n",
            "  Loaded input_2023_w05.csv: 254,779 rows\n",
            "  Loaded input_2023_w06.csv: 270,676 rows\n",
            "  Loaded input_2023_w07.csv: 233,597 rows\n",
            "  Loaded input_2023_w08.csv: 281,011 rows\n",
            "  Loaded input_2023_w09.csv: 252,796 rows\n",
            "  Loaded input_2023_w10.csv: 260,372 rows\n",
            "  Loaded input_2023_w11.csv: 243,413 rows\n",
            "  Loaded input_2023_w12.csv: 294,940 rows\n",
            "  Loaded input_2023_w13.csv: 233,755 rows\n",
            "  Loaded input_2023_w14.csv: 279,972 rows\n",
            "  Loaded input_2023_w15.csv: 281,820 rows\n",
            "  Loaded input_2023_w16.csv: 316,417 rows\n",
            "  Loaded input_2023_w17.csv: 277,582 rows\n",
            "  Loaded input_2023_w18.csv: 254,917 rows\n",
            "\n",
            "Total input rows: 4,880,579\n",
            "Total output rows: 562,936\n",
            "\n",
            "Input columns: ['game_id', 'play_id', 'player_to_predict', 'nfl_id', 'frame_id', 'play_direction', 'absolute_yardline_number', 'player_name', 'player_height', 'player_weight', 'player_birth_date', 'player_position', 'player_side', 'player_role', 'x', 'y', 's', 'a', 'dir', 'o', 'num_frames_output', 'ball_land_x', 'ball_land_y', 'week']\n",
            "\n",
            "Input sample:\n",
            "      game_id  play_id  player_to_predict  nfl_id  frame_id play_direction  \\\n",
            "0  2023090700      101              False   54527         1          right   \n",
            "1  2023090700      101              False   54527         2          right   \n",
            "2  2023090700      101              False   54527         3          right   \n",
            "3  2023090700      101              False   54527         4          right   \n",
            "4  2023090700      101              False   54527         5          right   \n",
            "\n",
            "   absolute_yardline_number player_name player_height  player_weight  ...  \\\n",
            "0                        42  Bryan Cook           6-1            210  ...   \n",
            "1                        42  Bryan Cook           6-1            210  ...   \n",
            "2                        42  Bryan Cook           6-1            210  ...   \n",
            "3                        42  Bryan Cook           6-1            210  ...   \n",
            "4                        42  Bryan Cook           6-1            210  ...   \n",
            "\n",
            "       x      y     s     a     dir       o  num_frames_output  ball_land_x  \\\n",
            "0  52.33  36.94  0.09  0.39  322.40  238.24                 21    63.259998   \n",
            "1  52.33  36.94  0.04  0.61  200.89  236.05                 21    63.259998   \n",
            "2  52.33  36.93  0.12  0.73  147.55  240.60                 21    63.259998   \n",
            "3  52.35  36.92  0.23  0.81  131.40  244.25                 21    63.259998   \n",
            "4  52.37  36.90  0.35  0.82  123.26  244.25                 21    63.259998   \n",
            "\n",
            "   ball_land_y  week  \n",
            "0        -0.22     1  \n",
            "1        -0.22     1  \n",
            "2        -0.22     1  \n",
            "3        -0.22     1  \n",
            "4        -0.22     1  \n",
            "\n",
            "[5 rows x 24 columns]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Section 2: Data Loading and Exploration\n",
        "# ============================================================================\n",
        "\n",
        "class NFLDataLoader:\n",
        "    \"\"\"Handles loading and preprocessing of NFL tracking data.\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.train_dir = Path(config.data_dir)\n",
        "        self.test_dir = Path(config.test_dir)\n",
        "        \n",
        "    def load_training_data(self, weeks: Optional[List[int]] = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Load training data from input and output files.\n",
        "        \n",
        "        Args:\n",
        "            weeks: List of week numbers to load (1-18). If None, load all.\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of (input_df, output_df)\n",
        "        \"\"\"\n",
        "        input_dfs = []\n",
        "        output_dfs = []\n",
        "        \n",
        "        # Find all input files\n",
        "        input_pattern = str(self.train_dir / 'input_2023_w*.csv')\n",
        "        input_files = sorted(glob.glob(input_pattern))\n",
        "        \n",
        "        if not input_files:\n",
        "            print(f\"No input files found at {input_pattern}\")\n",
        "            return None, None\n",
        "        \n",
        "        print(f\"Found {len(input_files)} input files\")\n",
        "        \n",
        "        for input_file in input_files:\n",
        "            # Extract week number\n",
        "            week_str = Path(input_file).stem.split('_w')[-1]\n",
        "            week = int(week_str)\n",
        "            \n",
        "            if weeks is not None and week not in weeks:\n",
        "                continue\n",
        "            \n",
        "            # Load input file\n",
        "            input_df = pd.read_csv(input_file)\n",
        "            input_df['week'] = week\n",
        "            input_dfs.append(input_df)\n",
        "            print(f\"  Loaded {Path(input_file).name}: {len(input_df):,} rows\")\n",
        "            \n",
        "            # Load corresponding output file\n",
        "            output_file = str(self.train_dir / f'output_2023_w{week_str}.csv')\n",
        "            if os.path.exists(output_file):\n",
        "                output_df = pd.read_csv(output_file)\n",
        "                output_df['week'] = week\n",
        "                output_dfs.append(output_df)\n",
        "        \n",
        "        if not input_dfs:\n",
        "            print(\"No data loaded!\")\n",
        "            return None, None\n",
        "        \n",
        "        all_input = pd.concat(input_dfs, ignore_index=True)\n",
        "        all_output = pd.concat(output_dfs, ignore_index=True) if output_dfs else None\n",
        "        \n",
        "        print(f\"\\nTotal input rows: {len(all_input):,}\")\n",
        "        if all_output is not None:\n",
        "            print(f\"Total output rows: {len(all_output):,}\")\n",
        "        \n",
        "        return all_input, all_output\n",
        "    \n",
        "    def load_test_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Load test data.\n",
        "        \n",
        "        Returns:\n",
        "            Tuple of (test_df, test_input_df)\n",
        "        \"\"\"\n",
        "        test_file = self.test_dir / 'test.csv'\n",
        "        test_input_file = self.test_dir / 'test_input.csv'\n",
        "        \n",
        "        test_df = None\n",
        "        test_input_df = None\n",
        "        \n",
        "        if test_file.exists():\n",
        "            test_df = pd.read_csv(test_file)\n",
        "            print(f\"Loaded test.csv: {len(test_df):,} rows\")\n",
        "        \n",
        "        if test_input_file.exists():\n",
        "            test_input_df = pd.read_csv(test_input_file)\n",
        "            print(f\"Loaded test_input.csv: {len(test_input_df):,} rows\")\n",
        "        \n",
        "        return test_df, test_input_df\n",
        "    \n",
        "    def create_training_samples(\n",
        "        self, \n",
        "        input_df: pd.DataFrame, \n",
        "        output_df: pd.DataFrame,\n",
        "        include_all_players: bool = False\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Create training samples by pairing input features with output targets.\n",
        "        \n",
        "        Args:\n",
        "            input_df: Pre-pass tracking data\n",
        "            output_df: Post-pass target positions\n",
        "            include_all_players: If False, only include players marked for prediction\n",
        "        \"\"\"\n",
        "        # Get last known state for each player before pass\n",
        "        last_state = input_df.sort_values('frame_id').groupby(\n",
        "            ['game_id', 'play_id', 'nfl_id']\n",
        "        ).last().reset_index()\n",
        "        \n",
        "        # Rename input columns to avoid confusion with targets\n",
        "        feature_cols = ['x', 'y', 's', 'a', 'dir', 'o']\n",
        "        rename_dict = {col: f'{col}_input' for col in feature_cols}\n",
        "        last_state = last_state.rename(columns=rename_dict)\n",
        "        \n",
        "        # Also rename frame_id from input to avoid collision\n",
        "        last_state = last_state.rename(columns={'frame_id': 'last_input_frame_id'})\n",
        "        \n",
        "        # Merge with targets\n",
        "        training_df = output_df.merge(\n",
        "            last_state,\n",
        "            on=['game_id', 'play_id', 'nfl_id'],\n",
        "            how='left'\n",
        "        )\n",
        "        \n",
        "        # Filter to only players we need to predict if requested\n",
        "        if not include_all_players and 'player_to_predict' in training_df.columns:\n",
        "            training_df = training_df[training_df['player_to_predict'] == True]\n",
        "        \n",
        "        # Create unique identifier\n",
        "        training_df['id'] = (\n",
        "            training_df['game_id'].astype(str) + '_' + \n",
        "            training_df['play_id'].astype(str) + '_' + \n",
        "            training_df['nfl_id'].astype(str) + '_' + \n",
        "            training_df['frame_id'].astype(str)\n",
        "        )\n",
        "        \n",
        "        return training_df\n",
        "\n",
        "\n",
        "# Initialize data loader\n",
        "data_loader = NFLDataLoader(config)\n",
        "\n",
        "# Load training data\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Loading training data...\")\n",
        "print(\"=\"*60)\n",
        "input_df, output_df = data_loader.load_training_data()\n",
        "\n",
        "if input_df is not None:\n",
        "    print(f\"\\nInput columns: {input_df.columns.tolist()}\")\n",
        "    print(f\"\\nInput sample:\")\n",
        "    print(input_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "INPUT DATA EXPLORATION\n",
            "============================================================\n",
            "\n",
            "Shape: (4880579, 24)\n",
            "\n",
            "Unique values:\n",
            "  Games: 272\n",
            "  Plays: 14108\n",
            "  Players: 1384\n",
            "\n",
            "Player positions: {'WR': 1063660, 'CB': 1056888, 'FS': 476865, 'TE': 417146, 'QB': 401007, 'SS': 392421, 'RB': 314918, 'ILB': 295593, 'OLB': 207429, 'MLB': 199983}\n",
            "\n",
            "Player sides: {'Defense': 2662657, 'Offense': 2217922}\n",
            "\n",
            "Player roles: {'Defensive Coverage': 2662657, 'Other Route Runner': 1424243, 'Targeted Receiver': 396914, 'Passer': 396765}\n",
            "\n",
            "Players to predict per play:\n",
            "  Mean: 92.39\n",
            "  Min: 9, Max: 567\n",
            "\n",
            "============================================================\n",
            "OUTPUT DATA EXPLORATION\n",
            "============================================================\n",
            "\n",
            "Shape: (562936, 7)\n",
            "\n",
            "Columns: ['game_id', 'play_id', 'nfl_id', 'frame_id', 'x', 'y', 'week']\n",
            "\n",
            "Sample:\n",
            "      game_id  play_id  nfl_id  frame_id      x      y  week\n",
            "0  2023090700      101   46137         1  56.22  17.28     1\n",
            "1  2023090700      101   46137         2  56.63  16.88     1\n",
            "2  2023090700      101   46137         3  57.06  16.46     1\n",
            "3  2023090700      101   46137         4  57.48  16.02     1\n",
            "4  2023090700      101   46137         5  57.91  15.56     1\n",
            "\n",
            "Frames per player per play:\n",
            "  Mean: 12.23\n",
            "  Min: 5, Max: 94\n"
          ]
        }
      ],
      "source": [
        "# Explore the data structure\n",
        "if input_df is not None:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INPUT DATA EXPLORATION\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    print(f\"\\nShape: {input_df.shape}\")\n",
        "    \n",
        "    print(f\"\\nUnique values:\")\n",
        "    print(f\"  Games: {input_df['game_id'].nunique()}\")\n",
        "    print(f\"  Plays: {input_df.groupby('game_id')['play_id'].nunique().sum()}\")\n",
        "    print(f\"  Players: {input_df['nfl_id'].nunique()}\")\n",
        "    \n",
        "    print(f\"\\nPlayer positions: {input_df['player_position'].value_counts().head(10).to_dict()}\")\n",
        "    print(f\"\\nPlayer sides: {input_df['player_side'].value_counts().to_dict()}\")\n",
        "    print(f\"\\nPlayer roles: {input_df['player_role'].value_counts().to_dict()}\")\n",
        "    \n",
        "    print(f\"\\nPlayers to predict per play:\")\n",
        "    pred_counts = input_df.groupby(['game_id', 'play_id'])['player_to_predict'].sum()\n",
        "    print(f\"  Mean: {pred_counts.mean():.2f}\")\n",
        "    print(f\"  Min: {pred_counts.min()}, Max: {pred_counts.max()}\")\n",
        "\n",
        "if output_df is not None:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"OUTPUT DATA EXPLORATION\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    print(f\"\\nShape: {output_df.shape}\")\n",
        "    print(f\"\\nColumns: {output_df.columns.tolist()}\")\n",
        "    print(f\"\\nSample:\")\n",
        "    print(output_df.head())\n",
        "    \n",
        "    print(f\"\\nFrames per player per play:\")\n",
        "    frame_counts = output_df.groupby(['game_id', 'play_id', 'nfl_id'])['frame_id'].count()\n",
        "    print(f\"  Mean: {frame_counts.mean():.2f}\")\n",
        "    print(f\"  Min: {frame_counts.min()}, Max: {frame_counts.max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Feature Engineering\n",
        "\n",
        "We create football-specific features that capture:\n",
        "- **Velocity & Acceleration**: Speed, direction changes\n",
        "- **Distance Features**: Distance to ball landing spot, to other players\n",
        "- **Angle Features**: Angle to ball, pursuit angles\n",
        "- **Separation Features**: Closest defender distance, separation metrics\n",
        "- **Context Features**: Field position, time since release"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Section 3: Feature Engineering\n",
        "# ============================================================================\n",
        "\n",
        "class FootballFeatureEngineer:\n",
        "    \"\"\"Creates football-specific features for player movement prediction.\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.scalers = {}\n",
        "        \n",
        "    def compute_velocity_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Compute velocity-based features.\"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        # Use input features (pre-pass state)\n",
        "        s_col = 's_input' if 's_input' in df.columns else 's'\n",
        "        dir_col = 'dir_input' if 'dir_input' in df.columns else 'dir'\n",
        "        \n",
        "        if s_col in df.columns and dir_col in df.columns:\n",
        "            # Velocity components from speed and direction\n",
        "            df['vx'] = df[s_col] * np.cos(np.radians(df[dir_col]))\n",
        "            df['vy'] = df[s_col] * np.sin(np.radians(df[dir_col]))\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def compute_distance_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Compute distance-based features.\"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        # Get position columns\n",
        "        x_col = 'x_input' if 'x_input' in df.columns else 'x'\n",
        "        y_col = 'y_input' if 'y_input' in df.columns else 'y'\n",
        "        \n",
        "        # Distance to ball landing spot\n",
        "        if 'ball_land_x' in df.columns and 'ball_land_y' in df.columns:\n",
        "            df['dist_to_ball_landing'] = np.sqrt(\n",
        "                (df[x_col] - df['ball_land_x'])**2 + \n",
        "                (df[y_col] - df['ball_land_y'])**2\n",
        "            )\n",
        "        \n",
        "        # Distance to line of scrimmage\n",
        "        if 'absolute_yardline_number' in df.columns:\n",
        "            df['dist_from_los'] = df[x_col] - df['absolute_yardline_number']\n",
        "        \n",
        "        # Distance to sidelines (field is 53.3 yards wide)\n",
        "        if y_col in df.columns:\n",
        "            df['dist_to_near_sideline'] = np.minimum(df[y_col], 53.3 - df[y_col])\n",
        "            df['dist_from_center'] = np.abs(df[y_col] - 26.65)\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def compute_angle_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Compute angle-based features.\"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        x_col = 'x_input' if 'x_input' in df.columns else 'x'\n",
        "        y_col = 'y_input' if 'y_input' in df.columns else 'y'\n",
        "        dir_col = 'dir_input' if 'dir_input' in df.columns else 'dir'\n",
        "        o_col = 'o_input' if 'o_input' in df.columns else 'o'\n",
        "        \n",
        "        # Angle to ball landing spot\n",
        "        if 'ball_land_x' in df.columns and 'ball_land_y' in df.columns:\n",
        "            df['angle_to_ball'] = np.degrees(np.arctan2(\n",
        "                df['ball_land_y'] - df[y_col],\n",
        "                df['ball_land_x'] - df[x_col]\n",
        "            ))\n",
        "            \n",
        "            if dir_col in df.columns:\n",
        "                # Difference between direction and angle to ball (pursuit angle)\n",
        "                df['pursuit_angle'] = np.abs(\n",
        "                    ((df[dir_col] - df['angle_to_ball'] + 180) % 360) - 180\n",
        "                )\n",
        "                \n",
        "                # Is player facing the ball? (within 45 degrees)\n",
        "                df['facing_ball'] = (df['pursuit_angle'] < 45).astype(int)\n",
        "        \n",
        "        # Orientation vs direction (body alignment)\n",
        "        if dir_col in df.columns and o_col in df.columns:\n",
        "            df['body_alignment'] = np.abs(\n",
        "                ((df[o_col] - df[dir_col] + 180) % 360) - 180\n",
        "            )\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def compute_player_context_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Compute player-specific context features.\"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        # Encode player side (offense vs defense)\n",
        "        if 'player_side' in df.columns:\n",
        "            df['is_offense'] = (df['player_side'] == 'Offense').astype(int)\n",
        "            df['is_defense'] = (df['player_side'] == 'Defense').astype(int)\n",
        "        \n",
        "        # Encode key positions\n",
        "        if 'player_position' in df.columns:\n",
        "            df['is_receiver'] = df['player_position'].isin(['WR', 'TE']).astype(int)\n",
        "            df['is_db'] = df['player_position'].isin(['CB', 'SS', 'FS', 'DB', 'S']).astype(int)\n",
        "            df['is_lb'] = df['player_position'].isin(['LB', 'ILB', 'OLB', 'MLB']).astype(int)\n",
        "        \n",
        "        # Play direction adjustment\n",
        "        if 'play_direction' in df.columns:\n",
        "            df['play_dir_right'] = (df['play_direction'] == 'right').astype(int)\n",
        "        \n",
        "        # Frame-based features\n",
        "        if 'frame_id' in df.columns and 'num_frames_output' in df.columns:\n",
        "            df['frame_progress'] = df['frame_id'] / df['num_frames_output'].clip(lower=1)\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def compute_target_relative_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Compute features relative to target position.\"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        x_col = 'x_input' if 'x_input' in df.columns else 'x'\n",
        "        y_col = 'y_input' if 'y_input' in df.columns else 'y'\n",
        "        \n",
        "        # Distance and angle to target (post-pass position we're predicting)\n",
        "        if 'x' in df.columns and 'y' in df.columns and x_col in df.columns:\n",
        "            df['target_displacement_x'] = df['x'] - df[x_col]\n",
        "            df['target_displacement_y'] = df['y'] - df[y_col]\n",
        "            df['target_displacement_dist'] = np.sqrt(\n",
        "                df['target_displacement_x']**2 + df['target_displacement_y']**2\n",
        "            )\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def engineer_features(\n",
        "        self, \n",
        "        df: pd.DataFrame, \n",
        "        is_training: bool = True\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"Apply all feature engineering steps.\"\"\"\n",
        "        print(\"Engineering features...\")\n",
        "        \n",
        "        if self.config.use_velocity_features:\n",
        "            print(\"  Computing velocity features...\")\n",
        "            df = self.compute_velocity_features(df)\n",
        "        \n",
        "        if self.config.use_distance_features:\n",
        "            print(\"  Computing distance features...\")\n",
        "            df = self.compute_distance_features(df)\n",
        "        \n",
        "        if self.config.use_angle_features:\n",
        "            print(\"  Computing angle features...\")\n",
        "            df = self.compute_angle_features(df)\n",
        "        \n",
        "        print(\"  Computing player context features...\")\n",
        "        df = self.compute_player_context_features(df)\n",
        "        \n",
        "        if is_training:\n",
        "            print(\"  Computing target relative features...\")\n",
        "            df = self.compute_target_relative_features(df)\n",
        "        \n",
        "        print(f\"  Feature engineering complete. Shape: {df.shape}\")\n",
        "        return df\n",
        "    \n",
        "    def get_feature_columns(self) -> List[str]:\n",
        "        \"\"\"Return list of feature column names for model input.\"\"\"\n",
        "        features = [\n",
        "            # Input position/motion features\n",
        "            'x_input', 'y_input', 's_input', 'a_input', 'dir_input', 'o_input',\n",
        "            # Velocity features\n",
        "            'vx', 'vy',\n",
        "            # Distance features\n",
        "            'dist_to_ball_landing', 'dist_from_los',\n",
        "            'dist_to_near_sideline', 'dist_from_center',\n",
        "            # Angle features\n",
        "            'angle_to_ball', 'pursuit_angle', 'facing_ball', 'body_alignment',\n",
        "            # Player context features\n",
        "            'is_offense', 'is_defense', 'is_receiver', 'is_db', 'is_lb',\n",
        "            'play_dir_right', 'frame_progress',\n",
        "            # Ball landing position\n",
        "            'ball_land_x', 'ball_land_y',\n",
        "            # Frame info\n",
        "            'frame_id', 'num_frames_output'\n",
        "        ]\n",
        "        return features\n",
        "    \n",
        "    def get_target_columns(self) -> List[str]:\n",
        "        \"\"\"Return list of target column names.\"\"\"\n",
        "        return ['x', 'y']\n",
        "\n",
        "\n",
        "# Initialize feature engineer\n",
        "feature_engineer = FootballFeatureEngineer(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Creating training samples...\n",
            "============================================================\n",
            "\n",
            "Training samples created: 562,936 rows\n",
            "Columns: ['game_id', 'play_id', 'nfl_id', 'frame_id', 'x', 'y', 'week_x', 'player_to_predict', 'last_input_frame_id', 'play_direction', 'absolute_yardline_number', 'player_name', 'player_height', 'player_weight', 'player_birth_date', 'player_position', 'player_side', 'player_role', 'x_input', 'y_input', 's_input', 'a_input', 'dir_input', 'o_input', 'num_frames_output', 'ball_land_x', 'ball_land_y', 'week_y', 'id']\n",
            "Engineering features...\n",
            "  Computing velocity features...\n",
            "  Computing distance features...\n",
            "  Computing angle features...\n",
            "  Computing player context features...\n",
            "  Computing target relative features...\n",
            "  Feature engineering complete. Shape: (562936, 49)\n",
            "\n",
            "After feature engineering: (562936, 49)\n",
            "\n",
            "Available features (27): ['x_input', 'y_input', 's_input', 'a_input', 'dir_input', 'o_input', 'vx', 'vy', 'dist_to_ball_landing', 'dist_from_los', 'dist_to_near_sideline', 'dist_from_center', 'angle_to_ball', 'pursuit_angle', 'facing_ball', 'body_alignment', 'is_offense', 'is_defense', 'is_receiver', 'is_db', 'is_lb', 'play_dir_right', 'frame_progress', 'ball_land_x', 'ball_land_y', 'frame_id', 'num_frames_output']\n",
            "      game_id  play_id  nfl_id  frame_id      x      y  week_x  \\\n",
            "0  2023090700      101   46137         1  56.22  17.28       1   \n",
            "1  2023090700      101   46137         2  56.63  16.88       1   \n",
            "2  2023090700      101   46137         3  57.06  16.46       1   \n",
            "3  2023090700      101   46137         4  57.48  16.02       1   \n",
            "4  2023090700      101   46137         5  57.91  15.56       1   \n",
            "\n",
            "   player_to_predict  last_input_frame_id play_direction  ...  is_offense  \\\n",
            "0               True                   26          right  ...           0   \n",
            "1               True                   26          right  ...           0   \n",
            "2               True                   26          right  ...           0   \n",
            "3               True                   26          right  ...           0   \n",
            "4               True                   26          right  ...           0   \n",
            "\n",
            "  is_defense is_receiver  is_db is_lb play_dir_right frame_progress  \\\n",
            "0          1           0      1     0              1       0.047619   \n",
            "1          1           0      1     0              1       0.095238   \n",
            "2          1           0      1     0              1       0.142857   \n",
            "3          1           0      1     0              1       0.190476   \n",
            "4          1           0      1     0              1       0.238095   \n",
            "\n",
            "  target_displacement_x  target_displacement_y  target_displacement_dist  \n",
            "0                  0.40                  -0.39                  0.558659  \n",
            "1                  0.81                  -0.79                  1.131459  \n",
            "2                  1.24                  -1.21                  1.732541  \n",
            "3                  1.66                  -1.65                  2.340534  \n",
            "4                  2.09                  -2.11                  2.969882  \n",
            "\n",
            "[5 rows x 49 columns]\n"
          ]
        }
      ],
      "source": [
        "# Create training dataset with features\n",
        "if input_df is not None and output_df is not None:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Creating training samples...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Create training samples by merging input (pre-pass) with output (targets)\n",
        "    training_df = data_loader.create_training_samples(\n",
        "        input_df, \n",
        "        output_df,\n",
        "        include_all_players=False  # Only predict for marked players\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nTraining samples created: {len(training_df):,} rows\")\n",
        "    print(f\"Columns: {training_df.columns.tolist()}\")\n",
        "    \n",
        "    # Engineer features\n",
        "    training_df = feature_engineer.engineer_features(training_df, is_training=True)\n",
        "    \n",
        "    print(f\"\\nAfter feature engineering: {training_df.shape}\")\n",
        "\n",
        "     # Show sample\n",
        "    feature_cols = feature_engineer.get_feature_columns()\n",
        "    available_features = [c for c in feature_cols if c in training_df.columns]\n",
        "    print(f\"\\nAvailable features ({len(available_features)}): {available_features}\")\n",
        "    \n",
        "    print(training_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Transformer Model (Time Dimension)\n",
        "\n",
        "The Transformer model processes sequences of player tracking data over time:\n",
        "- Positional encoding for temporal ordering\n",
        "- Multi-head self-attention for capturing temporal dependencies\n",
        "- Predicts future (x, y) positions based on past trajectory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformer model architecture defined.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Section 4: Transformer Model for Temporal Sequences\n",
        "# ============================================================================\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding for transformer.\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
        "        \n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "        \n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class PlayerMovementTransformer(nn.Module):\n",
        "    \"\"\"Transformer model for predicting player movement trajectories.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        d_model: int = 128,\n",
        "        n_heads: int = 8,\n",
        "        n_encoder_layers: int = 4,\n",
        "        dim_feedforward: int = 512,\n",
        "        dropout: float = 0.1,\n",
        "        max_seq_len: int = 100\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # Input projection\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Linear(input_dim, d_model),\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        \n",
        "        # Positional encoding\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len, dropout)\n",
        "        \n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=n_encoder_layers\n",
        "        )\n",
        "        \n",
        "        # Output heads for x and y prediction\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model // 2, 2)  # Predict (x, y)\n",
        "        )\n",
        "    \n",
        "    def forward(\n",
        "        self, \n",
        "        x: torch.Tensor, \n",
        "        mask: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        # Project input to model dimension\n",
        "        x = self.input_proj(x)\n",
        "        \n",
        "        # Add positional encoding\n",
        "        x = self.pos_encoder(x)\n",
        "        \n",
        "        # Create attention mask if provided\n",
        "        if mask is not None:\n",
        "            attn_mask = ~mask\n",
        "        else:\n",
        "            attn_mask = None\n",
        "        \n",
        "        # Transformer encoding\n",
        "        encoded = self.transformer_encoder(x, src_key_padding_mask=attn_mask)\n",
        "        \n",
        "        # Predict coordinates\n",
        "        predictions = self.output_head(encoded)\n",
        "        \n",
        "        return predictions\n",
        "\n",
        "\n",
        "print(\"Transformer model architecture defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Graph Neural Network (Space Dimension)\n",
        "\n",
        "The GNN captures player interactions within each frame:\n",
        "- Players are nodes with position/velocity features\n",
        "- Edges connect players based on proximity or team relationships\n",
        "- Message passing aggregates information from nearby players"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GNN model architecture defined.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Section 5: Graph Neural Network for Player Interactions\n",
        "# ============================================================================\n",
        "\n",
        "class PlayerInteractionGNN(nn.Module):\n",
        "    \"\"\"Graph Neural Network for modeling player interactions.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dim: int = 64,\n",
        "        output_dim: int = 2,\n",
        "        num_layers: int = 3,\n",
        "        heads: int = 4,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # Input projection\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "        \n",
        "        # GNN layers - use simple MLP-based message passing\n",
        "        self.gnn_layers = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.gnn_layers.append(nn.Sequential(\n",
        "                nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_dim, hidden_dim)\n",
        "            ))\n",
        "        \n",
        "        # Layer normalization\n",
        "        self.layer_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(hidden_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        # Output head\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, output_dim)\n",
        "        )\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        positions: torch.Tensor,\n",
        "        edge_threshold: float = 15.0\n",
        "    ) -> torch.Tensor:\n",
        "        # Project input\n",
        "        h = self.input_proj(x)\n",
        "        \n",
        "        # Build adjacency based on positions\n",
        "        diff = positions.unsqueeze(0) - positions.unsqueeze(1)\n",
        "        distances = torch.norm(diff, dim=-1)\n",
        "        adj = (distances < edge_threshold).float()\n",
        "        adj = adj / (adj.sum(dim=-1, keepdim=True) + 1e-8)\n",
        "        \n",
        "        # Message passing layers\n",
        "        for i, layer in enumerate(self.gnn_layers):\n",
        "            neighbor_features = torch.matmul(adj, h)\n",
        "            combined = torch.cat([h, neighbor_features], dim=-1)\n",
        "            h_new = layer(combined)\n",
        "            h = self.layer_norms[i](h + self.dropout(h_new))\n",
        "        \n",
        "        # Output prediction\n",
        "        output = self.output_head(h)\n",
        "        return output\n",
        "\n",
        "\n",
        "print(\"GNN model architecture defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: GBDT Model (Tabular Features)\n",
        "\n",
        "Gradient Boosted Decision Trees for tabular feature processing:\n",
        "- LightGBM/XGBoost for fast training\n",
        "- Handles engineered features effectively\n",
        "- Provides complementary predictions to deep models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GBDT model class defined.\n",
            "LightGBM available: True\n",
            "XGBoost available: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Section 6: GBDT Model for Tabular Features\n",
        "# ============================================================================\n",
        "\n",
        "class GBDTPredictor:\n",
        "    \"\"\"GBDT model for tabular feature prediction.\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.model_x = None\n",
        "        self.model_y = None\n",
        "        self.feature_cols = None\n",
        "        self.scaler = StandardScaler()\n",
        "        \n",
        "    def prepare_features(\n",
        "        self, \n",
        "        df: pd.DataFrame, \n",
        "        feature_cols: List[str]\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"Prepare features for GBDT model.\"\"\"\n",
        "        self.feature_cols = [c for c in feature_cols if c in df.columns]\n",
        "        \n",
        "        X = df[self.feature_cols].values\n",
        "        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        \n",
        "        return X\n",
        "    \n",
        "    def train(\n",
        "        self, \n",
        "        df: pd.DataFrame,\n",
        "        feature_cols: List[str],\n",
        "        target_col_x: str = 'x',\n",
        "        target_col_y: str = 'y',\n",
        "        val_df: Optional[pd.DataFrame] = None\n",
        "    ):\n",
        "        \"\"\"Train GBDT models for x and y prediction.\"\"\"\n",
        "        X = self.prepare_features(df, feature_cols)\n",
        "        y_x = df[target_col_x].values\n",
        "        y_y = df[target_col_y].values\n",
        "        \n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "        \n",
        "        # Prepare validation data if provided\n",
        "        X_val_scaled = None\n",
        "        y_val_x = None\n",
        "        y_val_y = None\n",
        "        if val_df is not None:\n",
        "            X_val = self.prepare_features(val_df, feature_cols)\n",
        "            X_val_scaled = self.scaler.transform(X_val)\n",
        "            y_val_x = val_df[target_col_x].values\n",
        "            y_val_y = val_df[target_col_y].values\n",
        "        \n",
        "        if HAS_LIGHTGBM:\n",
        "            print(\"Training LightGBM models...\")\n",
        "            \n",
        "            params = {\n",
        "                'objective': 'regression',\n",
        "                'metric': 'rmse',\n",
        "                'boosting_type': 'gbdt',\n",
        "                'num_leaves': 63,\n",
        "                'learning_rate': 0.05,\n",
        "                'feature_fraction': 0.8,\n",
        "                'bagging_fraction': 0.8,\n",
        "                'bagging_freq': 5,\n",
        "                'verbose': -1,\n",
        "                'n_jobs': -1,\n",
        "                'random_state': self.config.random_seed\n",
        "            }\n",
        "            \n",
        "            # Train model for X\n",
        "            train_data_x = lgb.Dataset(X_scaled, label=y_x)\n",
        "            valid_sets_x = [train_data_x]\n",
        "            if X_val_scaled is not None:\n",
        "                valid_data_x = lgb.Dataset(X_val_scaled, label=y_val_x)\n",
        "                valid_sets_x.append(valid_data_x)\n",
        "            \n",
        "            self.model_x = lgb.train(\n",
        "                params,\n",
        "                train_data_x,\n",
        "                num_boost_round=1000,\n",
        "                valid_sets=valid_sets_x,\n",
        "                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
        "            )\n",
        "            \n",
        "            # Train model for Y\n",
        "            train_data_y = lgb.Dataset(X_scaled, label=y_y)\n",
        "            valid_sets_y = [train_data_y]\n",
        "            if X_val_scaled is not None:\n",
        "                valid_data_y = lgb.Dataset(X_val_scaled, label=y_val_y)\n",
        "                valid_sets_y.append(valid_data_y)\n",
        "            \n",
        "            self.model_y = lgb.train(\n",
        "                params,\n",
        "                train_data_y,\n",
        "                num_boost_round=1000,\n",
        "                valid_sets=valid_sets_y,\n",
        "                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
        "            )\n",
        "            \n",
        "        elif HAS_XGBOOST:\n",
        "            print(\"Training XGBoost models...\")\n",
        "            \n",
        "            params = {\n",
        "                'objective': 'reg:squarederror',\n",
        "                'max_depth': 8,\n",
        "                'learning_rate': 0.05,\n",
        "                'n_estimators': 1000,\n",
        "                'subsample': 0.8,\n",
        "                'colsample_bytree': 0.8,\n",
        "                'random_state': self.config.random_seed,\n",
        "                'n_jobs': -1\n",
        "            }\n",
        "            \n",
        "            eval_set_x = [(X_scaled, y_x)]\n",
        "            eval_set_y = [(X_scaled, y_y)]\n",
        "            if X_val_scaled is not None:\n",
        "                eval_set_x.append((X_val_scaled, y_val_x))\n",
        "                eval_set_y.append((X_val_scaled, y_val_y))\n",
        "            \n",
        "            self.model_x = xgb.XGBRegressor(**params)\n",
        "            self.model_x.fit(\n",
        "                X_scaled, y_x,\n",
        "                eval_set=eval_set_x,\n",
        "                verbose=100\n",
        "            )\n",
        "            \n",
        "            self.model_y = xgb.XGBRegressor(**params)\n",
        "            self.model_y.fit(\n",
        "                X_scaled, y_y,\n",
        "                eval_set=eval_set_y,\n",
        "                verbose=100\n",
        "            )\n",
        "        else:\n",
        "            print(\"No GBDT library available. Skipping GBDT training.\")\n",
        "            return\n",
        "        \n",
        "        print(\"GBDT training complete.\")\n",
        "    \n",
        "    def predict(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Generate predictions using GBDT models.\"\"\"\n",
        "        if self.model_x is None or self.model_y is None:\n",
        "            raise ValueError(\"Models not trained. Call train() first.\")\n",
        "        \n",
        "        X = self.prepare_features(df, self.feature_cols)\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "        \n",
        "        pred_x = self.model_x.predict(X_scaled)\n",
        "        pred_y = self.model_y.predict(X_scaled)\n",
        "        \n",
        "        return pred_x, pred_y\n",
        "    \n",
        "    def get_feature_importance(self) -> pd.DataFrame:\n",
        "        \"\"\"Get feature importance from trained models.\"\"\"\n",
        "        if self.model_x is None:\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        if HAS_LIGHTGBM:\n",
        "            importance_x = self.model_x.feature_importance(importance_type='gain')\n",
        "            importance_y = self.model_y.feature_importance(importance_type='gain')\n",
        "        elif HAS_XGBOOST:\n",
        "            importance_x = self.model_x.feature_importances_\n",
        "            importance_y = self.model_y.feature_importances_\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': self.feature_cols,\n",
        "            'importance_x': importance_x,\n",
        "            'importance_y': importance_y,\n",
        "            'importance_avg': (importance_x + importance_y) / 2\n",
        "        }).sort_values('importance_avg', ascending=False)\n",
        "        \n",
        "        return importance_df\n",
        "    \n",
        "    def save(self, path: str):\n",
        "        \"\"\"Save model to disk.\"\"\"\n",
        "        os.makedirs(os.path.dirname(path) if os.path.dirname(path) else '.', exist_ok=True)\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'model_x': self.model_x,\n",
        "                'model_y': self.model_y,\n",
        "                'scaler': self.scaler,\n",
        "                'feature_cols': self.feature_cols\n",
        "            }, f)\n",
        "        print(f\"Model saved to {path}\")\n",
        "    \n",
        "    def load(self, path: str):\n",
        "        \"\"\"Load model from disk.\"\"\"\n",
        "        with open(path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            self.model_x = data['model_x']\n",
        "            self.model_y = data['model_y']\n",
        "            self.scaler = data['scaler']\n",
        "            self.feature_cols = data['feature_cols']\n",
        "        print(f\"Model loaded from {path}\")\n",
        "\n",
        "\n",
        "print(\"GBDT model class defined.\")\n",
        "print(f\"LightGBM available: {HAS_LIGHTGBM}\")\n",
        "print(f\"XGBoost available: {HAS_XGBOOST}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Training Pipeline\n",
        "\n",
        "Complete training pipeline that:\n",
        "1. Splits data into train/validation\n",
        "2. Engineers features\n",
        "3. Trains GBDT models\n",
        "4. Evaluates performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training GBDT Model\n",
            "============================================================\n",
            "\n",
            "Using 27 features\n",
            "\n",
            "Train size: 448,408, Validation size: 114,528\n",
            "Training LightGBM models...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[100]\ttraining's rmse: 1.2097\tvalid_1's rmse: 1.38163\n",
            "[200]\ttraining's rmse: 0.923389\tvalid_1's rmse: 1.23096\n",
            "[300]\ttraining's rmse: 0.813819\tvalid_1's rmse: 1.19725\n",
            "[400]\ttraining's rmse: 0.743824\tvalid_1's rmse: 1.18066\n",
            "[500]\ttraining's rmse: 0.691265\tvalid_1's rmse: 1.17234\n",
            "[600]\ttraining's rmse: 0.648905\tvalid_1's rmse: 1.16528\n",
            "[700]\ttraining's rmse: 0.613041\tvalid_1's rmse: 1.16186\n",
            "[800]\ttraining's rmse: 0.583623\tvalid_1's rmse: 1.1589\n",
            "[900]\ttraining's rmse: 0.555414\tvalid_1's rmse: 1.1577\n",
            "[1000]\ttraining's rmse: 0.532133\tvalid_1's rmse: 1.15367\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[999]\ttraining's rmse: 0.532386\tvalid_1's rmse: 1.15361\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[100]\ttraining's rmse: 0.977846\tvalid_1's rmse: 1.16205\n",
            "[200]\ttraining's rmse: 0.806096\tvalid_1's rmse: 1.07702\n",
            "[300]\ttraining's rmse: 0.720594\tvalid_1's rmse: 1.05814\n",
            "[400]\ttraining's rmse: 0.662198\tvalid_1's rmse: 1.04904\n",
            "[500]\ttraining's rmse: 0.617765\tvalid_1's rmse: 1.04333\n",
            "[600]\ttraining's rmse: 0.579261\tvalid_1's rmse: 1.04007\n",
            "[700]\ttraining's rmse: 0.547349\tvalid_1's rmse: 1.0376\n",
            "Early stopping, best iteration is:\n",
            "[682]\ttraining's rmse: 0.55317\tvalid_1's rmse: 1.0372\n",
            "GBDT training complete.\n",
            "\n",
            "=== Validation Evaluation ===\n",
            "Validation RMSE X: 1.1536\n",
            "Validation RMSE Y: 1.0372\n",
            "Validation RMSE Combined: 1.0970\n",
            "\n",
            "Top 10 Features:\n",
            "              feature  importance_x  importance_y  importance_avg\n",
            "              x_input  1.795214e+09  7.986324e+04    8.976470e+08\n",
            "              y_input  2.190712e+05  4.505400e+08    2.253795e+08\n",
            "          ball_land_x  4.429120e+08  7.239475e+04    2.214922e+08\n",
            "dist_to_near_sideline  1.830375e+05  6.894914e+07    3.456609e+07\n",
            "          ball_land_y  4.624107e+05  4.979685e+07    2.512963e+07\n",
            "     dist_from_center  7.390557e+04  4.555797e+07    2.281594e+07\n",
            "                   vx  1.700951e+05  2.893614e+07    1.455312e+07\n",
            "             frame_id  1.942369e+07  8.823099e+06    1.412340e+07\n",
            "        dist_from_los  2.633489e+07  2.614287e+05    1.329816e+07\n",
            "                   vy  2.468819e+07  9.598855e+04    1.239209e+07\n",
            "Model saved to ./model_output/gbdt_model.pkl\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Section 7: Training Pipeline\n",
        "# ============================================================================\n",
        "\n",
        "def train_gbdt_model(\n",
        "    training_df: pd.DataFrame,\n",
        "    feature_engineer: FootballFeatureEngineer,\n",
        "    config: Config,\n",
        "    val_ratio: float = 0.2\n",
        ") -> GBDTPredictor:\n",
        "    \"\"\"Train the GBDT model on prepared training data.\"\"\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Training GBDT Model\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Get feature columns\n",
        "    feature_cols = feature_engineer.get_feature_columns()\n",
        "    available_features = [c for c in feature_cols if c in training_df.columns]\n",
        "    print(f\"\\nUsing {len(available_features)} features\")\n",
        "    \n",
        "    # Split by plays for proper validation\n",
        "    unique_plays = training_df[['game_id', 'play_id']].drop_duplicates()\n",
        "    train_plays, val_plays = train_test_split(\n",
        "        unique_plays, \n",
        "        test_size=val_ratio, \n",
        "        random_state=config.random_seed\n",
        "    )\n",
        "    \n",
        "    train_df = training_df.merge(train_plays, on=['game_id', 'play_id'])\n",
        "    val_df = training_df.merge(val_plays, on=['game_id', 'play_id'])\n",
        "    \n",
        "    print(f\"\\nTrain size: {len(train_df):,}, Validation size: {len(val_df):,}\")\n",
        "    \n",
        "    # Initialize and train GBDT\n",
        "    gbdt = GBDTPredictor(config)\n",
        "    gbdt.train(\n",
        "        train_df, \n",
        "        available_features,\n",
        "        target_col_x='x',\n",
        "        target_col_y='y',\n",
        "        val_df=val_df\n",
        "    )\n",
        "    \n",
        "    # Evaluate on validation set\n",
        "    print(\"\\n=== Validation Evaluation ===\")\n",
        "    pred_x, pred_y = gbdt.predict(val_df)\n",
        "    \n",
        "    rmse_x = np.sqrt(mean_squared_error(val_df['x'], pred_x))\n",
        "    rmse_y = np.sqrt(mean_squared_error(val_df['y'], pred_y))\n",
        "    rmse_combined = np.sqrt(0.5 * (rmse_x**2 + rmse_y**2))\n",
        "    \n",
        "    print(f\"Validation RMSE X: {rmse_x:.4f}\")\n",
        "    print(f\"Validation RMSE Y: {rmse_y:.4f}\")\n",
        "    print(f\"Validation RMSE Combined: {rmse_combined:.4f}\")\n",
        "    \n",
        "    # Feature importance\n",
        "    importance_df = gbdt.get_feature_importance()\n",
        "    print(f\"\\nTop 10 Features:\")\n",
        "    print(importance_df.head(10).to_string(index=False))\n",
        "    \n",
        "    return gbdt, val_df, pred_x, pred_y\n",
        "\n",
        "\n",
        "# Train the model\n",
        "if 'training_df' in dir() and training_df is not None and len(training_df) > 0:\n",
        "    gbdt_model, val_df, val_pred_x, val_pred_y = train_gbdt_model(\n",
        "        training_df, \n",
        "        feature_engineer, \n",
        "        config\n",
        "    )\n",
        "    \n",
        "    # Save the model\n",
        "    os.makedirs('./model_output', exist_ok=True)\n",
        "    gbdt_model.save('./model_output/gbdt_model.pkl')\n",
        "else:\n",
        "    print(\"No training data available. Please load data first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Inference and Submission\n",
        "\n",
        "Integration with the Kaggle evaluation API for submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Inference function defined.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Section 8: Inference and Submission\n",
        "# ============================================================================\n",
        "\n",
        "# Global model storage\n",
        "GLOBAL_GBDT_MODEL = None\n",
        "GLOBAL_FEATURE_ENGINEER = None\n",
        "GLOBAL_CONFIG = None\n",
        "GLOBAL_INITIALIZED = False\n",
        "\n",
        "\n",
        "def initialize_model():\n",
        "    \"\"\"Initialize model for inference.\"\"\"\n",
        "    global GLOBAL_GBDT_MODEL, GLOBAL_FEATURE_ENGINEER, GLOBAL_CONFIG, GLOBAL_INITIALIZED\n",
        "    \n",
        "    if GLOBAL_INITIALIZED:\n",
        "        return\n",
        "    \n",
        "    GLOBAL_CONFIG = Config()\n",
        "    GLOBAL_FEATURE_ENGINEER = FootballFeatureEngineer(GLOBAL_CONFIG)\n",
        "    GLOBAL_GBDT_MODEL = GBDTPredictor(GLOBAL_CONFIG)\n",
        "    \n",
        "    # Try to load saved model\n",
        "    model_path = './model_output/gbdt_model.pkl'\n",
        "    if os.path.exists(model_path):\n",
        "        GLOBAL_GBDT_MODEL.load(model_path)\n",
        "        print(\"Model loaded successfully.\")\n",
        "    else:\n",
        "        print(\"No saved model found. Using physics-based baseline.\")\n",
        "    \n",
        "    GLOBAL_INITIALIZED = True\n",
        "\n",
        "\n",
        "def predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Main prediction function for the inference server.\n",
        "    \n",
        "    Args:\n",
        "        test: DataFrame with rows to predict (id, game_id, play_id, nfl_id, frame_id)\n",
        "        test_input: DataFrame with input features (pre-pass tracking data)\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with 'x' and 'y' predictions\n",
        "    \"\"\"\n",
        "    global GLOBAL_GBDT_MODEL, GLOBAL_FEATURE_ENGINEER, GLOBAL_CONFIG\n",
        "    \n",
        "    # Initialize on first call\n",
        "    initialize_model()\n",
        "    \n",
        "    # Convert to pandas for processing\n",
        "    test_pd = test.to_pandas() if isinstance(test, pl.DataFrame) else test\n",
        "    test_input_pd = test_input.to_pandas() if isinstance(test_input, pl.DataFrame) else test_input\n",
        "    \n",
        "    n_rows = len(test_pd)\n",
        "    \n",
        "    try:\n",
        "        # Get the last frame for each player from input (pre-pass state)\n",
        "        last_state = test_input_pd.sort_values('frame_id').groupby(\n",
        "            ['game_id', 'play_id', 'nfl_id']\n",
        "        ).last().reset_index()\n",
        "        \n",
        "        # Rename input columns\n",
        "        feature_cols = ['x', 'y', 's', 'a', 'dir', 'o']\n",
        "        rename_dict = {col: f'{col}_input' for col in feature_cols if col in last_state.columns}\n",
        "        last_state = last_state.rename(columns=rename_dict)\n",
        "        \n",
        "        # Also rename frame_id to avoid collision\n",
        "        last_state = last_state.rename(columns={'frame_id': 'last_input_frame_id'})\n",
        "        \n",
        "        # Merge test with input features\n",
        "        merged = test_pd.merge(\n",
        "            last_state,\n",
        "            on=['game_id', 'play_id', 'nfl_id'],\n",
        "            how='left'\n",
        "        )\n",
        "        \n",
        "        # Engineer features\n",
        "        merged = GLOBAL_FEATURE_ENGINEER.engineer_features(merged, is_training=False)\n",
        "        \n",
        "        # Try to use GBDT model\n",
        "        if GLOBAL_GBDT_MODEL.model_x is not None:\n",
        "            pred_x, pred_y = GLOBAL_GBDT_MODEL.predict(merged)\n",
        "        else:\n",
        "            # Physics-based baseline\n",
        "            dt = 0.1 * merged['frame_id']  # Time since pass release\n",
        "            \n",
        "            if 's_input' in merged.columns and 'dir_input' in merged.columns:\n",
        "                vx = merged['s_input'] * np.cos(np.radians(merged['dir_input']))\n",
        "                vy = merged['s_input'] * np.sin(np.radians(merged['dir_input']))\n",
        "                pred_x = merged['x_input'] + vx * dt\n",
        "                pred_y = merged['y_input'] + vy * dt\n",
        "            else:\n",
        "                pred_x = merged.get('x_input', np.zeros(n_rows) + 50).values\n",
        "                pred_y = merged.get('y_input', np.zeros(n_rows) + 26.65).values\n",
        "            \n",
        "            pred_x = np.array(pred_x)\n",
        "            pred_y = np.array(pred_y)\n",
        "        \n",
        "        # Clip predictions to field boundaries\n",
        "        pred_x = np.clip(pred_x, 0, 120)\n",
        "        pred_y = np.clip(pred_y, 0, 53.3)\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Prediction error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        \n",
        "        # Fallback predictions (field center)\n",
        "        pred_x = np.zeros(n_rows) + 50\n",
        "        pred_y = np.zeros(n_rows) + 26.65\n",
        "    \n",
        "    # Create prediction dataframe\n",
        "    predictions = pl.DataFrame({\n",
        "        'x': pred_x.tolist(),\n",
        "        'y': pred_y.tolist()\n",
        "    })\n",
        "    \n",
        "    assert len(predictions) == len(test), f\"Predictions length {len(predictions)} != test length {len(test)}\"\n",
        "    return predictions\n",
        "\n",
        "\n",
        "print(\"\\nInference function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Development Mode - Testing with Local Test Data\n",
            "============================================================\n",
            "Loaded test.csv: 5,837 rows\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded test_input.csv: 49,753 rows\n",
            "\n",
            "Test rows to predict: 5,837\n",
            "Test input rows: 49,753\n",
            "Model loaded from ./model_output/gbdt_model.pkl\n",
            "Model loaded successfully.\n",
            "Engineering features...\n",
            "  Computing velocity features...\n",
            "  Computing distance features...\n",
            "  Computing angle features...\n",
            "  Computing player context features...\n",
            "  Feature engineering complete. Shape: (5837, 42)\n",
            "\n",
            "Predictions shape: (5837, 2)\n",
            "\n",
            "Sample predictions:\n",
            "shape: (5, 2)\n",
            "\n",
            " x          y         \n",
            " ---        ---       \n",
            " f64        f64       \n",
            "\n",
            " 88.360978  34.287142 \n",
            " 88.483892  34.279449 \n",
            " 88.621921  34.35315  \n",
            " 88.900913  34.489986 \n",
            " 89.020976  34.614615 \n",
            "\n",
            "\n",
            "Predictions saved to ./test_predictions.csv\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Section 9: Test on Local Test Data\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Development Mode - Testing with Local Test Data\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load test data\n",
        "test_df, test_input_df = data_loader.load_test_data()\n",
        "\n",
        "if test_df is not None and test_input_df is not None:\n",
        "    print(f\"\\nTest rows to predict: {len(test_df):,}\")\n",
        "    print(f\"Test input rows: {len(test_input_df):,}\")\n",
        "    \n",
        "    # Convert to polars for prediction function\n",
        "    test_pl = pl.DataFrame(test_df)\n",
        "    test_input_pl = pl.DataFrame(test_input_df)\n",
        "    \n",
        "    # Run prediction\n",
        "    predictions = predict(test_pl, test_input_pl)\n",
        "    \n",
        "    print(f\"\\nPredictions shape: {predictions.shape}\")\n",
        "    print(f\"\\nSample predictions:\")\n",
        "    print(predictions.head())\n",
        "    \n",
        "    # Save predictions\n",
        "    pred_df = predictions.to_pandas()\n",
        "    pred_df['id'] = test_df['id']\n",
        "    pred_df = pred_df[['id', 'x', 'y']]\n",
        "    pred_df.to_csv('./test_predictions.csv', index=False)\n",
        "    print(f\"\\nPredictions saved to ./test_predictions.csv\")\n",
        "else:\n",
        "    print(\"No test data available.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 10: Visualization and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Model Analysis\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Section 10: Visualization and Analysis\n",
        "# ============================================================================\n",
        "\n",
        "def plot_field():\n",
        "    \"\"\"Create a football field plot.\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12, 5.33))\n",
        "    \n",
        "    ax.set_xlim(0, 120)\n",
        "    ax.set_ylim(0, 53.3)\n",
        "    ax.set_facecolor('#2e7d32')\n",
        "    \n",
        "    # Yard lines\n",
        "    for x in range(0, 121, 10):\n",
        "        ax.axvline(x, color='white', linewidth=0.5, alpha=0.5)\n",
        "    \n",
        "    # End zones\n",
        "    ax.axvline(10, color='white', linewidth=2)\n",
        "    ax.axvline(110, color='white', linewidth=2)\n",
        "    \n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "def plot_prediction_errors(val_df, pred_x, pred_y):\n",
        "    \"\"\"Plot prediction error distributions.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "    \n",
        "    error_x = pred_x - val_df['x'].values\n",
        "    error_y = pred_y - val_df['y'].values\n",
        "    error_dist = np.sqrt(error_x**2 + error_y**2)\n",
        "    \n",
        "    axes[0].hist(error_x, bins=50, edgecolor='black', alpha=0.7)\n",
        "    axes[0].axvline(0, color='red', linestyle='--')\n",
        "    axes[0].set_xlabel('X Error (yards)')\n",
        "    axes[0].set_title(f'X Error\\nMean: {error_x.mean():.2f}, Std: {error_x.std():.2f}')\n",
        "    \n",
        "    axes[1].hist(error_y, bins=50, edgecolor='black', alpha=0.7)\n",
        "    axes[1].axvline(0, color='red', linestyle='--')\n",
        "    axes[1].set_xlabel('Y Error (yards)')\n",
        "    axes[1].set_title(f'Y Error\\nMean: {error_y.mean():.2f}, Std: {error_y.std():.2f}')\n",
        "    \n",
        "    axes[2].hist(error_dist, bins=50, edgecolor='black', alpha=0.7, color='green')\n",
        "    axes[2].set_xlabel('Distance Error (yards)')\n",
        "    axes[2].set_title(f'Distance Error\\nMean: {error_dist.mean():.2f}, Std: {error_dist.std():.2f}')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_feature_importance(importance_df, top_n=15):\n",
        "    \"\"\"Plot feature importance.\"\"\"\n",
        "    if len(importance_df) == 0:\n",
        "        print(\"No feature importance data available.\")\n",
        "        return None\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    \n",
        "    top_features = importance_df.head(top_n)\n",
        "    \n",
        "    y_pos = np.arange(len(top_features))\n",
        "    ax.barh(y_pos, top_features['importance_avg'], align='center', color='steelblue')\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(top_features['feature'])\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_xlabel('Importance (Gain)')\n",
        "    ax.set_title(f'Top {top_n} Feature Importance')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "\n",
        "# Create visualizations if we have validation data\n",
        "if 'val_df' in dir() and 'val_pred_x' in dir():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Model Analysis\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Plot error distributions\n",
        "    fig = plot_prediction_errors(val_df, val_pred_x, val_pred_y)\n",
        "    plt.savefig('./error_distribution.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Plot feature importance\n",
        "    if 'gbdt_model' in dir():\n",
        "        importance_df = gbdt_model.get_feature_importance()\n",
        "        fig = plot_feature_importance(importance_df)\n",
        "        if fig:\n",
        "            plt.savefig('./feature_importance.png', dpi=150, bbox_inches='tight')\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook implements a comprehensive NFL player movement prediction solution:\n",
        "\n",
        "### Components\n",
        "1. **Data Loading**: Loads training data from `train/input_*.csv` and `train/output_*.csv`\n",
        "2. **Feature Engineering**: Football-specific features including velocity, distance to ball, angles, and player context\n",
        "3. **GBDT Model**: LightGBM/XGBoost for efficient tabular prediction\n",
        "4. **Inference Pipeline**: Ready for Kaggle evaluation API\n",
        "\n",
        "### Data Structure\n",
        "- **Input**: Pre-pass tracking data (position, velocity, direction before throw)\n",
        "- **Output**: Post-pass target positions (x, y to predict)\n",
        "- **Key columns**: `game_id`, `play_id`, `nfl_id`, `frame_id`, `ball_land_x`, `ball_land_y`\n",
        "\n",
        "### Next Steps for Improvement\n",
        "1. Add Transformer model for temporal sequence modeling\n",
        "2. Add GNN model for player interactions\n",
        "3. Implement ensemble of multiple models\n",
        "4. Add physics-based constraints\n",
        "5. Train position-specific models (WR, CB, etc.)\n",
        "6. Add trajectory smoothing"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
